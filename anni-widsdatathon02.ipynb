{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73478,"databundleVersionId":8121780,"sourceType":"competition"}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Description\n\n##Background: Equity in Healthcare\nHealthcare inequity is a global challenge. Addressing this challenge has an extensive positive impact on women’s health, which is key for societies and economies to thrive. This datathon is designed to help discover whether disparate treatments exist and to understand the drivers of those biases, such as demographic and societal factors.\n\nIn the first datathon challenge we explored the relationship between socio economic aspects that contribute to health equity. For this next challenge we’re building on that analysis to see how climate patterns impact access to healthcare.\n\n##Overview: The Dataset and Challenge\nGilead Sciences is the sponsor for the 2024 WiDS Datathon.The dataset originated from Health Verity, one of the largest healthcare data ecosystems in the US. It was enriched with third party geo-demographic data to provide views into the socio economic aspects that may contribute to health equity. For this challenge, the dataset was then further enriched with zip code level climate data.\n\n##Challenge task:\nPredicting the duration of time it takes for patients to receive metastatic cancer diagnosis.\n\n###Why is this important?\nMetastatic TNBC is considered the most aggressive TNBC and requires urgent and timely treatment. Unnecessary delays in diagnosis and subsequent treatment can have devastating effects in these difficult cancers. Differences in the wait time to get treatment is a good proxy for disparities in healthcare access.\n\nThe **primary goal** of building these models is to detect relationships between demographics of the patient with the likelihood of getting timely treatment. The **secondary goal** is to see if climate patterns impact proper diagnosis and treatment.","metadata":{"id":"9l7P9Q0kgTYY","execution":{"iopub.status.busy":"2024-06-07T18:24:31.629891Z","iopub.execute_input":"2024-06-07T18:24:31.630350Z","iopub.status.idle":"2024-06-07T18:24:31.645378Z","shell.execute_reply.started":"2024-06-07T18:24:31.630301Z","shell.execute_reply":"2024-06-07T18:24:31.642606Z"}}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:00:57.428796Z","iopub.execute_input":"2024-07-15T17:00:57.429542Z","iopub.status.idle":"2024-07-15T17:00:57.441103Z","shell.execute_reply.started":"2024-07-15T17:00:57.429502Z","shell.execute_reply":"2024-07-15T17:00:57.439933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.model_selection import train_test_split\n# import statsmodels.api as sm\n# from sklearn.preprocessing import LabelBinarizer","metadata":{"id":"iizQ1h9mhuaI","execution":{"iopub.status.busy":"2024-07-15T17:00:57.443408Z","iopub.execute_input":"2024-07-15T17:00:57.443771Z","iopub.status.idle":"2024-07-15T17:00:58.624155Z","shell.execute_reply.started":"2024-07-15T17:00:57.443740Z","shell.execute_reply":"2024-07-15T17:00:58.622379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data and Initial Data exploration","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/widsdatathon2024-challenge2/train.csv')\ntest = pd.read_csv('/kaggle/input/widsdatathon2024-challenge2/test.csv')\nsolution_template = pd.read_csv('/kaggle/input/widsdatathon2024-challenge2/solution_template.csv')\n# shuffle the training set - helps prevent bias during training\ntrain = train.reindex(np.random.permutation(train.index))\ndisplay(train.head(5))","metadata":{"id":"_PtsJf06kbIJ","outputId":"3bc6a4b1-2307-424c-f09f-990564463b15","execution":{"iopub.status.busy":"2024-07-15T17:00:58.625771Z","iopub.execute_input":"2024-07-15T17:00:58.626495Z","iopub.status.idle":"2024-07-15T17:00:59.086682Z","shell.execute_reply.started":"2024-07-15T17:00:58.626451Z","shell.execute_reply":"2024-07-15T17:00:59.085617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print out the size of the dataset\nprint(\"Number of rows and columns on training set are: \", train.shape)\nprint(\"Number of rows and columns on test set are: \", test.shape)","metadata":{"id":"aV1iNTFXm4Re","outputId":"147004ff-85b4-4f80-a1ad-17cbc693e37a","execution":{"iopub.status.busy":"2024-07-15T17:00:59.087904Z","iopub.execute_input":"2024-07-15T17:00:59.088197Z","iopub.status.idle":"2024-07-15T17:00:59.094393Z","shell.execute_reply.started":"2024-07-15T17:00:59.088172Z","shell.execute_reply":"2024-07-15T17:00:59.093180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(solution_template.head(5))\nprint(solution_template.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:00:59.097823Z","iopub.execute_input":"2024-07-15T17:00:59.098353Z","iopub.status.idle":"2024-07-15T17:00:59.110800Z","shell.execute_reply.started":"2024-07-15T17:00:59.098230Z","shell.execute_reply":"2024-07-15T17:00:59.109831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print a concise summary of train dataset.\ntrain.info()","metadata":{"id":"C46mNwR7nZ_Z","outputId":"7338fafe-6367-4a7d-fdb5-a02b7a7163e4","execution":{"iopub.status.busy":"2024-07-15T17:00:59.112311Z","iopub.execute_input":"2024-07-15T17:00:59.112847Z","iopub.status.idle":"2024-07-15T17:00:59.132004Z","shell.execute_reply.started":"2024-07-15T17:00:59.112780Z","shell.execute_reply":"2024-07-15T17:00:59.130790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the summary, we see that we have 11 categorical features","metadata":{"id":"6U9FkWwKnteG"}},{"cell_type":"code","source":"# Let's print a concise summary of test dataset.\ntest.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:00:59.133652Z","iopub.execute_input":"2024-07-15T17:00:59.134417Z","iopub.status.idle":"2024-07-15T17:00:59.152608Z","shell.execute_reply.started":"2024-07-15T17:00:59.134374Z","shell.execute_reply":"2024-07-15T17:00:59.151295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's print out the categorical columns and the numerical columns\n\n#Categorical:\ncat_cols = [col for col in train.columns if train[col].dtype == 'object']\n# print('Categorical Columns: ')\n# for value in cat_cols:\n#     print(value)\nprint('Categorical Columns: ', cat_cols)\n\n#Numerical:\nnum_cols = []\nfor col in train.columns:\n  if train[col].dtype == 'int64' or 'float64': #(you could also use != 'object')\n    num_cols.append(col)\nprint('Numerical Columns: ')\nfor val in num_cols:\n    print(val)\n","metadata":{"id":"dwkpCzhPny57","outputId":"fe265598-da07-464f-a641-d0c80ef84a1e","execution":{"iopub.status.busy":"2024-07-15T17:00:59.154277Z","iopub.execute_input":"2024-07-15T17:00:59.154753Z","iopub.status.idle":"2024-07-15T17:00:59.168992Z","shell.execute_reply.started":"2024-07-15T17:00:59.154718Z","shell.execute_reply":"2024-07-15T17:00:59.167763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find the number of unique values in each feature","metadata":{"id":"kTVP2Tnv93CB"}},{"cell_type":"code","source":"unique_values = pd.concat(\n    [\n        train.drop('metastatic_diagnosis_period', axis=1).nunique().rename('train'),\n        test.nunique().rename('test')\n    ],\n    axis=1\n)\n\ndisplay(unique_values.T)","metadata":{"id":"ERMy8P289-nw","outputId":"7e520e8d-b557-4318-cd4b-5d86c3934945","execution":{"iopub.status.busy":"2024-07-15T17:00:59.170551Z","iopub.execute_input":"2024-07-15T17:00:59.171050Z","iopub.status.idle":"2024-07-15T17:00:59.281776Z","shell.execute_reply.started":"2024-07-15T17:00:59.171005Z","shell.execute_reply":"2024-07-15T17:00:59.280622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting categorical features","metadata":{"id":"ta1UivM-hvVq"}},{"cell_type":"code","source":"#Let's plot some of the categorical columns\n\n# Creating a barplot for 'payer_ type'\n\n# Setting the width and height of the figure\nplt.figure(figsize=(9, 7))\n\n# Bar chart\nbars = train['payer_type'].value_counts().plot(kind='bar')\n\n# Setting the value count and fontsize of each bar labels (2103, 882...)\nbars.bar_label(bars.containers[0],fontsize = 13)\n\n# Setting the size of the xticks, yticks, and rotating the tick labels to be horizontal\nbars.tick_params(labelsize = 10, labelrotation = 360)\nplt.show()","metadata":{"id":"QWE_73_jp_kl","outputId":"8e9fae8d-75db-4f44-c237-ff177886e972","execution":{"iopub.status.busy":"2024-07-15T17:00:59.283211Z","iopub.execute_input":"2024-07-15T17:00:59.283544Z","iopub.status.idle":"2024-07-15T17:00:59.542099Z","shell.execute_reply.started":"2024-07-15T17:00:59.283515Z","shell.execute_reply":"2024-07-15T17:00:59.540792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the number of people using medicare advantage and medicaid are pretty similar. Over half of the patients used commercial payments. Would be nice to explore what commercial here means. Does it mean cash or company sponsored insurances like Anthem and Blue Cross Blue shield?","metadata":{"id":"CT7XOJhF-DhC"}},{"cell_type":"code","source":"# Creating a Bar Chart of Patient Race\n\n# Setting the width and height of the figure\nplt.figure(figsize=(12, 6))\n\n# Bar chart for train data\nplt.subplot(121)\nbars = train['patient_race'].value_counts().plot(kind='bar')\n# You can also use seaborn to create the bars:\n# bars = sns.barplot(y = train.index, x = train['patient_race'])\n# Setting the size of inside bar labels\nbars.bar_label(bars.containers[0],fontsize = 10)\n# Setting the title, xlabel, ylabel for the plot and their sizes\nbars.axes.set_title(\"Number of Patients by Race in Train Data\",fontsize = 14)\nbars.set_xlabel(\"Patient Race\",fontsize = 12)\nbars.set_ylabel(\"Number of Patients\",fontsize = 12)\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 360)\n\n# Bar chart for test data\nplt.subplot(122)\nbars = test['patient_race'].value_counts().plot(kind='bar')\n# You can also use seaborn to create the bars:\n# bars = sns.barplot(y = train.index, x = train['patient_race'])\n# Setting the size of inside bar labels\nbars.bar_label(bars.containers[0],fontsize = 10)\n# Setting the title, xlabel, ylabel for the plot and their sizes\nbars.axes.set_title(\"Number of Patients by Race in Test Data\",fontsize = 14)\nbars.set_xlabel(\"Patient Race\",fontsize = 12)\nbars.set_ylabel(\"Number of Patients\",fontsize = 12)\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 360)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"IDbnUcO34fqu","outputId":"149ce235-8741-4a0d-bd83-fb503834049d","execution":{"iopub.status.busy":"2024-07-15T17:00:59.543522Z","iopub.execute_input":"2024-07-15T17:00:59.543918Z","iopub.status.idle":"2024-07-15T17:01:00.158366Z","shell.execute_reply.started":"2024-07-15T17:00:59.543886Z","shell.execute_reply":"2024-07-15T17:01:00.157262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Patient race data is heavily skewed with the number of patients identifying as white exceeding all other races combined.","metadata":{"id":"DL3tPFRv_njs"}},{"cell_type":"code","source":"# Creating a Bar Plot of Patient State\n\n# Setting the width and height of the figure\nplt.figure(figsize=(12, 9))\n\n# Bar chart\nbars = train['patient_state'].value_counts().plot(kind='bar')\n\n# # Setting the size of inside bar labels\nbars.bar_label(bars.containers[0],fontsize = 8)\n\n# # Setting the title, xlabel, ylabel for the plot and their sizes\nbars.axes.set_title(\"Number of Patients by State in Train Data\",fontsize = 15)\n# bars.set_xlabel(\"Patient Race\",fontsize = 15)\n# bars.set_ylabel(\"Number of Patients\",fontsize = 15)\n# # Setting the size of the xticks and yticks\n# bars.tick_params(labelsize = 10, labelrotation = 360)\nplt.show()","metadata":{"id":"SkdUBXhfAP_1","outputId":"8109ef5f-8b7e-4f51-9210-1ebea5ebff56","execution":{"iopub.status.busy":"2024-07-15T17:01:00.160058Z","iopub.execute_input":"2024-07-15T17:01:00.160449Z","iopub.status.idle":"2024-07-15T17:01:00.857834Z","shell.execute_reply.started":"2024-07-15T17:01:00.160415Z","shell.execute_reply":"2024-07-15T17:01:00.856643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the barplot, California(CA) has the highest number of patients followed by New York(NY) and Texas(TX).\n\nOne reasonable explanation could be because they are the most populated states.\n\nIt would also be great to look into other factors such as food quality, air quality etc. that may explain why these three states have the highest number of cancer patients.","metadata":{"id":"1XE5GTXcD1Ss"}},{"cell_type":"code","source":"# Creating a Bar Plot of Patient Region\n\n# Setting the width and height of the subplots\nplt.figure(figsize=(12, 6))\n\n# Bar chart for train data\nplt.subplot(121) #(using plt.subplot to create subplots)\nbars = train['Region'].value_counts().plot(kind='bar')\n# Setting the size of inside bar labels\nbars.bar_label(bars.containers[0],fontsize = 10)\n# Setting the title, xlabel, ylabel for the plot and their sizes\nbars.axes.set_title(\"Number of Patients by Region in Train Data\",fontsize = 14)\nbars.set_xlabel(\"Patient Region\",fontsize = 12)\nbars.set_ylabel(\"Number of Patients\",fontsize = 12)\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 360)\n\n# Bar chart for test data\nplt.subplot(122)\nbars = test['Region'].value_counts().plot(kind='bar')\n# Setting the size of inside bar labels\nbars.bar_label(bars.containers[0],fontsize = 10)\n# Setting the title, xlabel, ylabel for the plot and their sizes\nbars.axes.set_title(\"Number of Patients by Region in Test Data\",fontsize = 14)\nbars.set_xlabel(\"Patient Region\",fontsize = 12)\nbars.set_ylabel(\"Number of Patients\",fontsize = 12)\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 360)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"J8JvJKB1HsWH","outputId":"21d92e05-e248-4632-8471-8829fb7d0afe","execution":{"iopub.status.busy":"2024-07-15T17:01:00.859370Z","iopub.execute_input":"2024-07-15T17:01:00.859822Z","iopub.status.idle":"2024-07-15T17:01:01.456464Z","shell.execute_reply.started":"2024-07-15T17:01:00.859782Z","shell.execute_reply":"2024-07-15T17:01:01.454993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Northeast region seems to have the least amount of cancer patients. The data doesn't describe exactly what states are considered to be in each region, so it can be hard to draw further value from the plot above.","metadata":{"id":"XKZcPT_7IsTq"}},{"cell_type":"code","source":"# Plotting the distribution of patient races by region\n\n# Since the patient race is a categorical feature, we'll try using a countplot variation to plot the distribution.\n# A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable.\n# The basic API and options are identical to those for barplot(), so you can compare counts across nested variables.\n\n# Setting the width and height of the subplots\nplt.figure(figsize=(15, 9))\n\n# Plot for train data\nplt.subplot(211) #(no. of rows = 2, no. of cols = 1, plot no.1)\nbars = sns.countplot(x = train['patient_race'], hue = train['Region'], palette = ['blue', 'deeppink','darkviolet','lime'])\n\n# Setting the size of inside bar labels\nbars.bar_label(bars.containers[0],fontsize = 10)\nbars.bar_label(bars.containers[1],fontsize = 10)\nbars.bar_label(bars.containers[2],fontsize = 10)\nbars.bar_label(bars.containers[3],fontsize = 10)\n# Setting the title, xlabel, ylabel for the plot and their sizes\nbars.axes.set_title(\"Number of Patient Race by Region(Train)\",fontsize = 14)\nbars.set_xlabel(\"Patient Race\",fontsize = 12)\nbars.set_ylabel(\"Number of Patients\",fontsize = 12)\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 360)\n\n\n# Plot for test data\nplt.subplot(212) #(no. of rows = 2, no. of cols = 1, plot no.2)\nbars = sns.countplot(x = test['patient_race'], hue = test['Region'], palette = ['blue', 'deeppink','darkviolet','lime'])\n\n# Setting the size of inside bar labels\nbars.bar_label(bars.containers[0],fontsize = 10)\nbars.bar_label(bars.containers[1],fontsize = 10)\nbars.bar_label(bars.containers[2],fontsize = 10)\nbars.bar_label(bars.containers[3],fontsize = 10)\n# Setting the title, xlabel, ylabel for the plot and their sizes\nbars.axes.set_title(\"Number of Patient Race by Region(Test)\",fontsize = 14)\nbars.set_xlabel(\"Patient Race\",fontsize = 12)\nbars.set_ylabel(\"Number of Patients\",fontsize = 12)\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 360)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"RWUxLO62rkxo","outputId":"f9cece19-1dd8-44cd-99cf-938c3afbd4ce","execution":{"iopub.status.busy":"2024-07-15T17:01:01.462576Z","iopub.execute_input":"2024-07-15T17:01:01.463552Z","iopub.status.idle":"2024-07-15T17:01:02.541709Z","shell.execute_reply.started":"2024-07-15T17:01:01.463514Z","shell.execute_reply":"2024-07-15T17:01:02.540506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting numerical features","metadata":{"id":"r7cSmig4ou_q"}},{"cell_type":"code","source":"# Plotting some numerical features\n\n# Creating a Histogram for Patient Age\n\n# Create subplots(using plt.subplots) with 1 row and 2 columns and their width and height(figsize)\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n# Plot for patient_age in train data\naxes[0].hist(train['patient_age'], bins=15, color='skyblue', edgecolor='black')\n# Setting the title, xlabel, ylabel for the plot and their sizes\naxes[0].set_title('Patient Age Distribution in Train Data', fontsize = 14)\naxes[0].set_xlabel('Patient Age (yrs)', fontsize = 12)\naxes[0].set_ylabel('Frequency', fontsize = 12)\n\n# Plot for patient_age in test data\naxes[1].hist(test['patient_age'], bins=15, color='hotpink', edgecolor='black')\n# Setting the title, xlabel, ylabel for the plot and their sizes\naxes[1].set_title('Patient Age Distribution in Test Data', fontsize = 14)\naxes[1].set_xlabel('Patient Age (yrs)', fontsize = 12)\naxes[1].set_ylabel('Frequency', fontsize = 12)\n\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"9zSkfQoEZLde","outputId":"601a5481-ba8b-4a9b-f979-925df6554e09","execution":{"iopub.status.busy":"2024-07-15T17:01:02.543174Z","iopub.execute_input":"2024-07-15T17:01:02.543616Z","iopub.status.idle":"2024-07-15T17:01:03.152856Z","shell.execute_reply.started":"2024-07-15T17:01:02.543576Z","shell.execute_reply":"2024-07-15T17:01:03.151749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Income distribution\n\n# Create subplots(using plt.subplots) with 1 row and 2 columns and their width and height(figsize)\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n# Plot for patient_age in train data\n# axes[0].hist(train['patient_age'], bins=15, color='skyblue', edgecolor='black')\nincome_median_train = train['income_household_median'].median()\naxes[0].hist(train['income_household_median'], bins=15, color='skyblue', edgecolor='black')\n\n# Setting the title, xlabel, ylabel for the plot and their sizes\naxes[0].set_title('Median Income Distribution in train data', fontsize = 14)\naxes[0].set_xlabel('Household Income($)', fontsize = 12)\naxes[0].set_ylabel('Frequency', fontsize = 12)\n\n# Plot for patient_age in test data\nincome_median_test = test['income_household_median'].median()\naxes[1].hist(test['income_household_median'], bins=15, color='hotpink', edgecolor='black')\n# Setting the title, xlabel, ylabel for the plot and their sizes\naxes[1].set_title('Median Income Distribution in test data', fontsize = 14)\naxes[1].set_xlabel('Household Income($)', fontsize = 12)\naxes[1].set_ylabel('Frequency', fontsize = 12)\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"yoJuHoh5-YSy","outputId":"297672d3-f1b8-480d-e94f-8524b3c8d646","execution":{"iopub.status.busy":"2024-07-15T17:01:03.154231Z","iopub.execute_input":"2024-07-15T17:01:03.154686Z","iopub.status.idle":"2024-07-15T17:01:03.736315Z","shell.execute_reply.started":"2024-07-15T17:01:03.154646Z","shell.execute_reply":"2024-07-15T17:01:03.735144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More Exploratory Analysis","metadata":{"id":"r2bkzuE8o_lQ"}},{"cell_type":"code","source":"train['patient_gender'].info()","metadata":{"id":"0jVTaK_5BR2T","outputId":"0731a528-4f12-4f82-c956-6a94439f1cce","execution":{"iopub.status.busy":"2024-07-15T17:01:03.738020Z","iopub.execute_input":"2024-07-15T17:01:03.738479Z","iopub.status.idle":"2024-07-15T17:01:03.749980Z","shell.execute_reply.started":"2024-07-15T17:01:03.738438Z","shell.execute_reply":"2024-07-15T17:01:03.748781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"brww2DN_nLKg"}},{"cell_type":"markdown","source":"From the info above, we see that all patients identified as Female. So we do not need to plot a chart for the patient gender.","metadata":{"id":"7WYI3T6vBgPA"}},{"cell_type":"markdown","source":"Let's take a deeper look into the metastatic diagnosis period","metadata":{"id":"P2iCJUaKykCI"}},{"cell_type":"code","source":"train['metastatic_diagnosis_period'].describe()","metadata":{"id":"MJzi0rt3yvjJ","outputId":"02146fd8-f8ad-4fec-dfb3-a526761684bf","execution":{"iopub.status.busy":"2024-07-15T17:01:03.751906Z","iopub.execute_input":"2024-07-15T17:01:03.752389Z","iopub.status.idle":"2024-07-15T17:01:03.769275Z","shell.execute_reply.started":"2024-07-15T17:01:03.752346Z","shell.execute_reply":"2024-07-15T17:01:03.767591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of the metastatic diagnosis period feature\nplt.figure(figsize = (10, 6))\ntarget_dist = train['metastatic_diagnosis_period'].plot(kind='hist', figsize=(10, 4), xlabel='metastatic_diagnosis_period', ylabel='Count')","metadata":{"id":"Gt387KQA8m8A","outputId":"ea755773-7b7b-42ea-9d24-21df3aecd641","execution":{"iopub.status.busy":"2024-07-15T17:01:03.771039Z","iopub.execute_input":"2024-07-15T17:01:03.771524Z","iopub.status.idle":"2024-07-15T17:01:04.070817Z","shell.execute_reply.started":"2024-07-15T17:01:03.771481Z","shell.execute_reply":"2024-07-15T17:01:04.069747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the mean and median of this feature for each race","metadata":{"id":"hggJztVMy8gM"}},{"cell_type":"code","source":"# Let's find the count for when the metastatic diagnosis period was zero days\n\nzero_count = (train['metastatic_diagnosis_period'] == 0).sum()\nprint(\"The amount of metastatic diagnoses that took 0 days are: \", zero_count)","metadata":{"id":"oAFaz4Hm3UMq","outputId":"c23a8675-d1d5-43e6-f885-de1ea42177db","execution":{"iopub.status.busy":"2024-07-15T17:01:04.072436Z","iopub.execute_input":"2024-07-15T17:01:04.072941Z","iopub.status.idle":"2024-07-15T17:01:04.080982Z","shell.execute_reply.started":"2024-07-15T17:01:04.072894Z","shell.execute_reply":"2024-07-15T17:01:04.079735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take this a step further, and check zero_count for each patient race\n\n# Creating a new dataframe where metastatic diagnosis period is 0 days.\nzero_days_train = train[train['metastatic_diagnosis_period'] == 0]\n\n# Plotting this in relation to patient race\nplt.figure(figsize=(10, 6)) # Setting the size of the plot figure\n\nbars = zero_days_train['patient_race'].value_counts().plot(kind='bar')\nbars.bar_label(bars.containers[0],fontsize = 10)\nbars.axes.set_title(\"No. of Patients with diagnosis period of 0 days\",fontsize = 14)\nbars.set_xlabel(\"Patient Race\",fontsize = 12)\nbars.set_ylabel(\"Number of Patients\",fontsize = 12)\n\nplt.show()","metadata":{"id":"0XTBrrR34qKP","outputId":"43a14edc-9857-4ee1-cb3e-66539145ab1e","execution":{"iopub.status.busy":"2024-07-15T17:01:04.082640Z","iopub.execute_input":"2024-07-15T17:01:04.083551Z","iopub.status.idle":"2024-07-15T17:01:04.325645Z","shell.execute_reply.started":"2024-07-15T17:01:04.083509Z","shell.execute_reply":"2024-07-15T17:01:04.324577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that white patients have the quickest metastatic diagnosis period compared to other races.\n\nThis could also be because white patient population is the largest amongst other race populations.","metadata":{"id":"_1pSroOE5QCn"}},{"cell_type":"code","source":"# Mean metastatic diagnosis period for each race\n\n# Setting the size of the plot figure\nplt.figure(figsize=(9, 6))\n\n# Plotting the bar graph\nax = sns.barplot(data=train, x='patient_race', y='metastatic_diagnosis_period',  errorbar = None)  # ci=None disables error bars\nplt.xlabel('Patient Race')\nplt.ylabel('Mean Metastatic Diagnosis Period(in days)')\nplt.title('Mean Metastatic Diagnosis Period by Patient Race (Train)')\nplt.xticks(rotation = 360)  # Rotating x-axis labels for better readability\n\n# Annotate each bar with its corresponding mean value\nfor bar in ax.patches:\n    ax.annotate(format(bar.get_height(), '.2f'),\n                 (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n                 ha ='center', va ='center',\n                 size = 10, xytext = (0, 8),\n                 textcoords = 'offset points')\n\nplt.tight_layout()  # Adjust layout to prevent label cutoff\nplt.show()","metadata":{"id":"3tshRrb4zCHm","outputId":"5fccfd4a-de41-464b-f281-0d25a0b51b17","execution":{"iopub.status.busy":"2024-07-15T17:01:04.326957Z","iopub.execute_input":"2024-07-15T17:01:04.327295Z","iopub.status.idle":"2024-07-15T17:01:04.599220Z","shell.execute_reply.started":"2024-07-15T17:01:04.327265Z","shell.execute_reply":"2024-07-15T17:01:04.597936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting a summary of the 'breast_cancer_diagnosis_code' feature\nprint(\"Statistical summary of 'breast_cancer_diagnosis_code' \\n\")\ntrain['breast_cancer_diagnosis_code'].info()\nprint(\"\\nFirst five rows:\")\ntrain['breast_cancer_diagnosis_code'].head()\n","metadata":{"id":"EXakIuDieza0","outputId":"e48fa022-a535-41f7-e42c-630be094cf09","execution":{"iopub.status.busy":"2024-07-15T17:01:04.600845Z","iopub.execute_input":"2024-07-15T17:01:04.601314Z","iopub.status.idle":"2024-07-15T17:01:04.616085Z","shell.execute_reply.started":"2024-07-15T17:01:04.601273Z","shell.execute_reply":"2024-07-15T17:01:04.614889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at the Top 10 metastatic diagnosis codes\n\ncode_counts = train['metastatic_cancer_diagnosis_code'].value_counts()\ntop_10_codes = code_counts.head(10)\n# print(\"Top 10 Metastatic Diagnosis Codes:\")\n# print(top_10_codes)\n\nplt.figure(figsize = (10, 8))\nax = sns.barplot(x = top_10_codes.index,hue = top_10_codes.index,y = top_10_codes.values, palette = 'viridis')\nfor i in range(10): # since we are only showing top 10 codes.\n  ax.bar_label(ax.containers[i], fontsize = 10) # prints the count of each bar/container.\nplt.title('Top 10 Metastatic Cancer Codes', fontsize = 14)\nplt.xlabel('Metastatic Cancer Code', fontsize = 12)\nplt.ylabel('Count', fontsize = 12)\nplt.show()","metadata":{"id":"ut3_bCqCIKBh","outputId":"346c6a50-6822-4958-b0ab-74ae22343aeb","execution":{"iopub.status.busy":"2024-07-15T17:01:04.617532Z","iopub.execute_input":"2024-07-15T17:01:04.617995Z","iopub.status.idle":"2024-07-15T17:01:05.837798Z","shell.execute_reply.started":"2024-07-15T17:01:04.617963Z","shell.execute_reply":"2024-07-15T17:01:05.836454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation between Features","metadata":{"id":"ZIowTAYPpgf4"}},{"cell_type":"code","source":"# Let's start with patient race vs payer type\n\n# Plot for Train Data\n\n# We'll create a daframe by crossing patient race and payer type using crosstab function in pandas then plot the dataframe\nbars = pd.crosstab(train['patient_race'],train['payer_type']).plot(kind = \"bar\",stacked = True, color =['#24b1d1', '#ae24d1', '#00FF00'] ) #stacked ensures we plot them ontop of each other\n\n# Setting the labels inside the stacked bars\n# Since we have stacked bars, we will use the 3 lines below to find counts of types of paymemnt in each race\n# Alternatively, one can use a for loop to pass through containers and save count.\nbars.bar_label(bars.containers[0],fontsize = 7, label_type= 'center') #counts on commercial\nbars.bar_label(bars.containers[1],fontsize = 7, label_type= 'center', color = 'white') #counts on medicaid\nbars.bar_label(bars.containers[2],fontsize = 7, label_type= 'center', color = 'red') #counts on medicare advantage\n\n# Setting the title and xlabel for the plot and their sizes\nbars.axes.set_title(\"Types of payments by Patient Race in Train Data\",fontsize = 13)\nbars.set_xlabel(\"Patient Race\",fontsize = 12)\nbars.set_ylabel(\"Count\",fontsize = 12)\n\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 0)\n\nplt.show()","metadata":{"id":"mR1fIYqI1WH-","outputId":"5f545dd9-cf19-4c97-e2f7-e296ff4d7b28","execution":{"iopub.status.busy":"2024-07-15T17:01:05.839153Z","iopub.execute_input":"2024-07-15T17:01:05.839535Z","iopub.status.idle":"2024-07-15T17:01:06.199334Z","shell.execute_reply.started":"2024-07-15T17:01:05.839503Z","shell.execute_reply":"2024-07-15T17:01:06.198116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot above, we first acknowledge that the white race population is the highest, so their payment type counts will overall be larger than the other patients.\n\nWhite patients seem to be fairly distributed between the 3 payment options with more using medicare advantage.\n\nBlack patients more significantly use medicaid as the form of payment rather than Medicare Advantage and Commercial.\n\nAsian patients use commercial and medicare at similar rates with the least being medicare advantage.\n\nHispanic patients use medicare about twice as much as they use commercial or medicare advantage as forms of payment.\n\nOthe patients use commercial about 4 times as much as they use medicare and about 6 times as much as medicare advantage.\n\nInterestingly, we can also point out that only white patients use medicare advantage more than the other payments Whereas Asian, Black and Hispanic patients use medicaid the most.","metadata":{"id":"0RmcBMESE34A"}},{"cell_type":"code","source":"# Plot for Test Data\n\n# We'll create a daframe by crossing patient race and payer type using crosstab function in pandas then plot the dataframe\nbars = pd.crosstab(test['patient_race'],train['payer_type']).plot(kind = \"bar\",stacked = True, color =['#24b1d1', '#ae24d1', '#00FF00'] ) #stacked ensures we plot them ontop of each other\n\n# Setting the labels inside the stacked bars\nbars.bar_label(bars.containers[0],fontsize = 7, label_type= 'center') #counts on commercial\nbars.bar_label(bars.containers[1],fontsize = 7, label_type= 'center', color = 'white') #counts on medicaid\nbars.bar_label(bars.containers[2],fontsize = 7, label_type= 'center', color = 'red') #counts on medicare advantage\n\n# Setting the title and xlabel for the plot and their sizes\nbars.axes.set_title(\"Types of payments by Patient Race in Test Data\",fontsize = 13)\nbars.set_xlabel(\"Patient Race\",fontsize = 12)\nbars.set_ylabel(\"Count\",fontsize = 12)\n\n# Setting the size of the xticks and yticks\nbars.tick_params(labelsize = 10, labelrotation = 0)\n\nplt.show()","metadata":{"id":"wko4AQzpp2TK","outputId":"013063ba-1ec3-4772-b87f-0f4a27881e5c","execution":{"iopub.status.busy":"2024-07-15T17:01:06.200967Z","iopub.execute_input":"2024-07-15T17:01:06.201534Z","iopub.status.idle":"2024-07-15T17:01:06.559536Z","shell.execute_reply.started":"2024-07-15T17:01:06.201489Z","shell.execute_reply":"2024-07-15T17:01:06.558427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In test data plot above, we see that commercial is the most used form of payment across all patient races.","metadata":{"id":"q4bgblmbqNG8"}},{"cell_type":"markdown","source":"We know that socio-economic factors may play an impact into  ones' quality of health from the food they are likely to consume to the health services available to them. Let's plot a correlation heatmap to analyze the relationship between income, poverty and unemployment.","metadata":{"id":"zRy_AEyZ8lnT"}},{"cell_type":"code","source":"# Correlastion Heatmap\ncorrelation_map_variables = ['income_household_median', 'poverty', 'unemployment_rate']\n\ndata = train[correlation_map_variables]\n\ncorr_matrix = data.corr()\n\nplt.figure(figsize = (8,6))\nsns.heatmap(corr_matrix, annot=True, cmap=\"BuPu\", vmin=-1, vmax=1)\nplt.title('Correlation Heatmap for Income, Unemployment Rate, and Poverty')\nplt.show()","metadata":{"id":"qX9BE_Dq9Ql3","outputId":"24ded61b-5a50-4019-f00e-2d23572054f2","execution":{"iopub.status.busy":"2024-07-15T17:01:06.561148Z","iopub.execute_input":"2024-07-15T17:01:06.561619Z","iopub.status.idle":"2024-07-15T17:01:06.842666Z","shell.execute_reply.started":"2024-07-15T17:01:06.561576Z","shell.execute_reply":"2024-07-15T17:01:06.841567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Poverty in our dataset is measured by the median value of owner occupied homes.\n\nHousehold income and Poverty have a high correlation of -0.69, showing what we know that the higher the household income, less likely the poverty.\n\nSimilarly, Unemployment Rate and Poverty have a high correlation of 0.68, showing that unemployed people are more likely to fall into poverty.\n","metadata":{"id":"pU5z3rwg9n08"}},{"cell_type":"markdown","source":"Seeing the presence of multi collinearilty between these features,we'll drop poverty from our dataset.","metadata":{"id":"fjKOMUty-qTj"}},{"cell_type":"code","source":"# Next we'll look at the relationship btw bmi and patient races.\n# Before we plot, it's important to note that both of these features have over 50% missing values, so the plot\n# insights may be inaccurately represented.\n\n# train[['patient_race','bmi']].mean().plot(kind='bar', color=\"red\")\n# train.groupby(['patient_race'])[\"bmi\"].head(5)\n# race_bmi = train[[\"patient_race\", \"bmi\"]]\n# race_bmi.plot(kind = \"bar\")\n# race_bmi = train[(train['patient_race']!= 'NaN') & (train['bmi']!= 'NaN')]\n# race_bmi_updated = race_bmi[[\"patient_race\", \"bmi\"]]\n# race_bmi_updated.plot(kind = \"bar\")","metadata":{"id":"kD673XeaHXOS","execution":{"iopub.status.busy":"2024-07-15T17:01:06.844357Z","iopub.execute_input":"2024-07-15T17:01:06.844785Z","iopub.status.idle":"2024-07-15T17:01:06.851920Z","shell.execute_reply.started":"2024-07-15T17:01:06.844751Z","shell.execute_reply":"2024-07-15T17:01:06.850167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# More correlations that could be further explored\n\n# poverty vs patient_race\n# income_household vs metastatic_diagnosis_period\n# health_uninsured vs metastatic_diagnosis_period\n# limited_english vs metastatic_diagnosis_period\n# patient_age vs metastatic_diagnosis_period","metadata":{"id":"co3xU6QdXgnH","execution":{"iopub.status.busy":"2024-07-15T17:01:06.853593Z","iopub.execute_input":"2024-07-15T17:01:06.853999Z","iopub.status.idle":"2024-07-15T17:01:06.864472Z","shell.execute_reply.started":"2024-07-15T17:01:06.853966Z","shell.execute_reply":"2024-07-15T17:01:06.863392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Values","metadata":{"id":"sWuK9Mithbjz"}},{"cell_type":"code","source":"# Let's check for missing values in our dataset. Any changes we implement on train set on this section, we'll also implement on test set.\n\nmissing_values = ( (train.isna().sum()) / len(train) ) * 100 # gives us percentage of missing values in each column\nmissing_values = missing_values.drop(missing_values[missing_values == 0].index) # exclude columns with 0 missing values\nmissing_values = missing_values.sort_values(ascending = False) # Sort missing values in descending order\nmissing_values.head(15)","metadata":{"id":"vp05VOWNhnUK","outputId":"31853258-c493-4c00-b6f0-fca09e6122d1","execution":{"iopub.status.busy":"2024-07-15T17:01:06.865888Z","iopub.execute_input":"2024-07-15T17:01:06.866349Z","iopub.status.idle":"2024-07-15T17:01:06.896483Z","shell.execute_reply.started":"2024-07-15T17:01:06.866309Z","shell.execute_reply":"2024-07-15T17:01:06.895373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Features with over 10% missing values are: \\n\", missing_values[missing_values > 10])","metadata":{"id":"OKK072Tx0BU0","outputId":"47ec8618-1aec-4d98-f6bb-f8a8afdb2be9","execution":{"iopub.status.busy":"2024-07-15T17:01:06.897993Z","iopub.execute_input":"2024-07-15T17:01:06.898515Z","iopub.status.idle":"2024-07-15T17:01:06.907121Z","shell.execute_reply.started":"2024-07-15T17:01:06.898471Z","shell.execute_reply":"2024-07-15T17:01:06.905950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that metastatic first novel treatment has over 99% missing values. With that much data missing, the 2 metastatic features provide close to zero valuable info. Therefore, we will be deleting them\n\nbmi is a valuable feature, but it has over 68% missing values. We will take a closer look to see if we will choose to keep it and do feature engineering or drop it.\n\nFor the rest (patient race, payer type, etc..) we will be replacing missing values with averages.","metadata":{"id":"7qJcEmZ4z3K-"}},{"cell_type":"markdown","source":"Let's look at the percentage of missing values by columns in both train and test sets","metadata":{"id":"H7puvozi9EBU"}},{"cell_type":"code","source":"pct_missing = pd.concat(\n    [\n        (train.drop('metastatic_diagnosis_period', axis=1).isna().sum() / len(train)).rename('train'),\n        (test.isna().sum() / len(test)).rename('test')\n    ],\n    axis=1\n)\n\nwith pd.option_context('display.precision', 4):\n    display(pct_missing.T)","metadata":{"id":"54hBlYUD9DDn","outputId":"fbaddace-2620-488a-a3fa-d74c1745a99d","execution":{"iopub.status.busy":"2024-07-15T17:01:06.908655Z","iopub.execute_input":"2024-07-15T17:01:06.909157Z","iopub.status.idle":"2024-07-15T17:01:06.962625Z","shell.execute_reply.started":"2024-07-15T17:01:06.909112Z","shell.execute_reply":"2024-07-15T17:01:06.961369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"QnFRSbA80KyW"}},{"cell_type":"markdown","source":"We'll start by deleting features with the most missing values. From the code above, we see that metastatic_first_novel_treatment and metastatic_first_novel_treatment_type have over 99% missing values. We won't derive any value from them.\n\nWe also saw that patient gender has only one unique value(female), so we'll remove it as well since it doesn't derive much insight.","metadata":{"id":"gEPpetu20aFb"}},{"cell_type":"code","source":"print(\"Shape before dropping columns: \\n\")\nprint(\"train shape\", train.shape)\nprint(\"test shape\", test.shape)","metadata":{"id":"JGYWyiNg1lY6","outputId":"9e412fe6-5b7a-4f59-cb69-b4ff1b59586b","execution":{"iopub.status.busy":"2024-07-15T17:01:06.964249Z","iopub.execute_input":"2024-07-15T17:01:06.964708Z","iopub.status.idle":"2024-07-15T17:01:06.971163Z","shell.execute_reply.started":"2024-07-15T17:01:06.964668Z","shell.execute_reply":"2024-07-15T17:01:06.969775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = train.drop(columns = ['patient_gender', 'metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type'])\ndf_test = test.drop(columns = ['patient_gender', 'metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type'], axis=1)","metadata":{"id":"HsZyaL0U0NSx","execution":{"iopub.status.busy":"2024-07-15T17:01:06.973338Z","iopub.execute_input":"2024-07-15T17:01:06.973824Z","iopub.status.idle":"2024-07-15T17:01:06.991712Z","shell.execute_reply.started":"2024-07-15T17:01:06.973762Z","shell.execute_reply":"2024-07-15T17:01:06.990410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape after dropping columns: \\n\")\nprint(\"df_train shape: \", df_train.shape)\nprint(\"df_test shape: \", df_test.shape)","metadata":{"id":"wsvZR50-3NSY","outputId":"6d2c2f9d-3ff3-4f51-ac84-48c579f507f4","execution":{"iopub.status.busy":"2024-07-15T17:01:06.993112Z","iopub.execute_input":"2024-07-15T17:01:06.993468Z","iopub.status.idle":"2024-07-15T17:01:06.999744Z","shell.execute_reply.started":"2024-07-15T17:01:06.993437Z","shell.execute_reply":"2024-07-15T17:01:06.998511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will check for any duplicated rows in our dataset. If we find any, we'll drop the rows.\n\n","metadata":{"id":"jcxnIRo9LzEz"}},{"cell_type":"code","source":"duplicate_train = df_train[df_train.duplicated() == True]\nduplicate_test = df_test[df_test.duplicated() == True]\nprint(duplicate_train.shape)\nprint(duplicate_test.shape)","metadata":{"id":"CZcnpOu4NmMQ","outputId":"370bdb69-429d-4fb7-f99d-c37a7e5bd84a","execution":{"iopub.status.busy":"2024-07-15T17:01:07.001159Z","iopub.execute_input":"2024-07-15T17:01:07.001586Z","iopub.status.idle":"2024-07-15T17:01:07.135987Z","shell.execute_reply.started":"2024-07-15T17:01:07.001531Z","shell.execute_reply":"2024-07-15T17:01:07.134803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see both train and test sets do not have any duplicate rows","metadata":{"id":"Wsix1vFKOeIp"}},{"cell_type":"markdown","source":"Now, we'll detect and fix outliers/inconsistencies. They appear in the following ways:\n\n\n\n*   **Location** based features: patient_zip3, patient_state, Division, Region\n<!-- * **Population** based features: population, veteran -->\n*   **Temperature** based features: Average of Jan-13, Average of Feb-13...Average of Dec-18\n*   Patient **Diagnosis** feature: breast_cancer_diagnosis_code\n\n\n\n\n\n","metadata":{"id":"wcnODpVhHg5C"}},{"cell_type":"markdown","source":"Let's start by checking if there's more than one state per patient_zip.","metadata":{"id":"EIyLHQ8JJhGY"}},{"cell_type":"markdown","source":"Test Set","metadata":{"id":"6ErFjpSJSAnR"}},{"cell_type":"code","source":"df_test[['patient_state','patient_zip3']].drop_duplicates().groupby('patient_zip3').count().sort_values('patient_state', ascending=False).head(10)","metadata":{"id":"kzHEIYnsR29T","outputId":"ff5d6d04-016e-4f6a-f1cb-ea83f61c4e4f","execution":{"iopub.status.busy":"2024-07-15T17:01:07.137385Z","iopub.execute_input":"2024-07-15T17:01:07.137755Z","iopub.status.idle":"2024-07-15T17:01:07.153607Z","shell.execute_reply.started":"2024-07-15T17:01:07.137723Z","shell.execute_reply":"2024-07-15T17:01:07.152453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train Set","metadata":{"id":"XeuoNCNuSFNb"}},{"cell_type":"code","source":"df_train[['patient_state','patient_zip3']].drop_duplicates().groupby('patient_zip3').count().sort_values('patient_state', ascending=False).head(10)","metadata":{"id":"4vGuNxhVI33a","outputId":"c581fecd-61e3-42f6-b247-1d782bb0a595","execution":{"iopub.status.busy":"2024-07-15T17:01:07.154990Z","iopub.execute_input":"2024-07-15T17:01:07.155353Z","iopub.status.idle":"2024-07-15T17:01:07.170785Z","shell.execute_reply.started":"2024-07-15T17:01:07.155320Z","shell.execute_reply":"2024-07-15T17:01:07.169750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that zip 630 and zip 864 are in 2 states\n\nLet's take a step further and see what these states are for each of the zipcodes. We'll use df.loc property to access a group of rows and columns for the 2 zipcodes.\n\nUsing df.loc, we'll create a conditional that returns a boolean series with column labels specified","metadata":{"id":"CV2clXH1QI1I"}},{"cell_type":"code","source":"df_train.loc[df_train['patient_zip3'].isin([630, 864]), ['patient_state', 'patient_zip3']].drop_duplicates()","metadata":{"id":"pypz6ceKHqoY","outputId":"b87f3dd5-3d66-426d-c263-d6cf6fa27dcc","execution":{"iopub.status.busy":"2024-07-15T17:01:07.172033Z","iopub.execute_input":"2024-07-15T17:01:07.172399Z","iopub.status.idle":"2024-07-15T17:01:07.188726Z","shell.execute_reply.started":"2024-07-15T17:01:07.172369Z","shell.execute_reply":"2024-07-15T17:01:07.187072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So zip 630 shows up in both MO and IL, whereas zip 864 shows up in both AZ and CA.\n\nAccording to [united states zipcodes](https://www.unitedstateszipcodes.org/) , zip 630 belongs to MO and zip 864 belongs to AZ.\n\nWe'll fix the data to reflect that bu using np.where","metadata":{"id":"AnqSBcrZS-xf"}},{"cell_type":"code","source":"df_train['patient_state'] = np.where(df_train['patient_zip3'] == 630, 'MO', df_train['patient_state'])\ndf_train['patient_state'] = np.where(df_train['patient_zip3'] == 864, 'AZ', df_train['patient_state'])","metadata":{"id":"NLmRxdFOZy2r","execution":{"iopub.status.busy":"2024-07-15T17:01:07.190690Z","iopub.execute_input":"2024-07-15T17:01:07.191293Z","iopub.status.idle":"2024-07-15T17:01:07.200750Z","shell.execute_reply.started":"2024-07-15T17:01:07.191246Z","shell.execute_reply":"2024-07-15T17:01:07.198999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at state allocation to division. In reality, each state belongs to a single division. Let's check for any inconsistencies","metadata":{"id":"OR-jvQTVadui"}},{"cell_type":"markdown","source":"Test Data","metadata":{"id":"Q-KlMDjpbTWS"}},{"cell_type":"code","source":"df_test[['patient_state','Division']].drop_duplicates().groupby('patient_state').count().sort_values('Division', ascending=False).head(10)","metadata":{"id":"3z7QZp0cbO13","outputId":"f7c2dc74-2228-4943-9916-b6428c7d1f0f","execution":{"iopub.status.busy":"2024-07-15T17:01:07.212591Z","iopub.execute_input":"2024-07-15T17:01:07.213218Z","iopub.status.idle":"2024-07-15T17:01:07.231417Z","shell.execute_reply.started":"2024-07-15T17:01:07.213144Z","shell.execute_reply":"2024-07-15T17:01:07.230261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train set","metadata":{"id":"aqV9it8GcKfH"}},{"cell_type":"code","source":"df_train[['patient_state','Division']].drop_duplicates().groupby('patient_state').count().sort_values('Division', ascending=False).head(10)","metadata":{"id":"X4T3oNeZb4kK","outputId":"2668986d-9c05-4f2a-866f-96a9d495db45","execution":{"iopub.status.busy":"2024-07-15T17:01:07.233152Z","iopub.execute_input":"2024-07-15T17:01:07.233507Z","iopub.status.idle":"2024-07-15T17:01:07.251782Z","shell.execute_reply.started":"2024-07-15T17:01:07.233477Z","shell.execute_reply":"2024-07-15T17:01:07.250468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the 2 divisions where MO appears","metadata":{"id":"hQjAdDiNcMth"}},{"cell_type":"code","source":"df_train.loc[df_train['patient_state'] == 'MO', ['patient_state', 'Division']].drop_duplicates()","metadata":{"id":"Mh2kHiO7cMWj","outputId":"4cbc3b9b-e9b6-4deb-d888-3ab582c80e6e","execution":{"iopub.status.busy":"2024-07-15T17:01:07.253013Z","iopub.execute_input":"2024-07-15T17:01:07.253350Z","iopub.status.idle":"2024-07-15T17:01:07.270273Z","shell.execute_reply.started":"2024-07-15T17:01:07.253322Z","shell.execute_reply":"2024-07-15T17:01:07.269005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missouri(MO) seems to appear in both West North Central and East North Central.\n\nLet's count the values for each division","metadata":{"id":"ozBGXFQ7dcvQ"}},{"cell_type":"code","source":"df_train.loc[df_train['patient_state'] == 'MO'].value_counts('Division')","metadata":{"id":"zkmZt6QbdwjC","outputId":"b50c9e0e-4d49-4ca9-e532-c696aefe844a","execution":{"iopub.status.busy":"2024-07-15T17:01:07.271754Z","iopub.execute_input":"2024-07-15T17:01:07.272111Z","iopub.status.idle":"2024-07-15T17:01:07.284803Z","shell.execute_reply.started":"2024-07-15T17:01:07.272080Z","shell.execute_reply":"2024-07-15T17:01:07.283614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since all but 1 are in West North Central, we'll assume it was an error for the single one to be in East North CentraL. So, we'll change it to West North Central.","metadata":{"id":"44ll-XFWeA4R"}},{"cell_type":"code","source":"df_train['Division'] = np.where(df_train['patient_state'] == 'MO', 'West North Central', df_train['Division'])","metadata":{"id":"_S8rFaPNd73U","execution":{"iopub.status.busy":"2024-07-15T17:01:07.286095Z","iopub.execute_input":"2024-07-15T17:01:07.286474Z","iopub.status.idle":"2024-07-15T17:01:07.295053Z","shell.execute_reply.started":"2024-07-15T17:01:07.286445Z","shell.execute_reply":"2024-07-15T17:01:07.293934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.loc[df_train['patient_state'] == 'MO', ['patient_state', 'Division']].drop_duplicates()","metadata":{"id":"WKcNDF9mzkiX","outputId":"985c5f4d-f386-4529-c188-ea517c3cd527","execution":{"iopub.status.busy":"2024-07-15T17:01:07.296437Z","iopub.execute_input":"2024-07-15T17:01:07.296931Z","iopub.status.idle":"2024-07-15T17:01:07.314626Z","shell.execute_reply.started":"2024-07-15T17:01:07.296889Z","shell.execute_reply":"2024-07-15T17:01:07.313498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[['patient_state','Division']].drop_duplicates().groupby('patient_state').count().sort_values('Division', ascending=False).head(5)","metadata":{"id":"Rm33AyYIytNC","outputId":"e9a0cd5e-2c5a-4a7d-8f54-d5f895e8cf08","execution":{"iopub.status.busy":"2024-07-15T17:01:07.315995Z","iopub.execute_input":"2024-07-15T17:01:07.316416Z","iopub.status.idle":"2024-07-15T17:01:07.333223Z","shell.execute_reply.started":"2024-07-15T17:01:07.316383Z","shell.execute_reply":"2024-07-15T17:01:07.331903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also check for 'region'.","metadata":{"id":"kLsFES9X0Gc4"}},{"cell_type":"markdown","source":"","metadata":{"id":"2QgR9VmP0MBj"}},{"cell_type":"code","source":"# Test Set\ndf_test[['patient_state', 'Region']].drop_duplicates().groupby('patient_state').count().sort_values('Region', ascending=False).head(5)","metadata":{"outputId":"6d3ef6a5-bc81-4bf3-8a61-dd57254333b1","id":"7T2dQ0j50f-N","execution":{"iopub.status.busy":"2024-07-15T17:01:07.334474Z","iopub.execute_input":"2024-07-15T17:01:07.334815Z","iopub.status.idle":"2024-07-15T17:01:07.489891Z","shell.execute_reply.started":"2024-07-15T17:01:07.334786Z","shell.execute_reply":"2024-07-15T17:01:07.488470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Set\ndf_train[['patient_state','Region']].drop_duplicates().groupby('patient_state').count().sort_values('Region', ascending=False).head(5)","metadata":{"outputId":"7c0831db-8c22-442e-b1c4-fdb2c6dec0f4","id":"E31DX9aC0dPg","execution":{"iopub.status.busy":"2024-07-15T17:01:07.491232Z","iopub.execute_input":"2024-07-15T17:01:07.491697Z","iopub.status.idle":"2024-07-15T17:01:07.509585Z","shell.execute_reply.started":"2024-07-15T17:01:07.491652Z","shell.execute_reply":"2024-07-15T17:01:07.508516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Region feature is consistent in both train and test sets. No need for any changes","metadata":{"id":"Y0TxzWca1egO"}},{"cell_type":"markdown","source":"","metadata":{"id":"s88SteFS1k5P"}},{"cell_type":"markdown","source":"Next features we'll explore are Temperature related.","metadata":{"id":"SuKhLb1nT3BH"}},{"cell_type":"code","source":"df_train['Average of Jan-13'].head()","metadata":{"id":"Ixh-kRIVQ9QV","outputId":"50b40783-5640-49fc-827b-73bca36ecc4d","execution":{"iopub.status.busy":"2024-07-15T17:01:07.510914Z","iopub.execute_input":"2024-07-15T17:01:07.511279Z","iopub.status.idle":"2024-07-15T17:01:07.519976Z","shell.execute_reply.started":"2024-07-15T17:01:07.511248Z","shell.execute_reply":"2024-07-15T17:01:07.518619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get a monthly average instead of day averages in each month. This will reduce the number of features in our data set, and make training the ML model more efficient. We should see a decrease in no. of features after this.","metadata":{"id":"CG8us6l1UAKf"}},{"cell_type":"code","source":"df_train.shape","metadata":{"id":"e2sm2oE5YsjG","outputId":"8d38922d-d3ee-483f-947c-02672cc07942","execution":{"iopub.status.busy":"2024-07-15T17:01:07.521381Z","iopub.execute_input":"2024-07-15T17:01:07.521823Z","iopub.status.idle":"2024-07-15T17:01:07.531687Z","shell.execute_reply.started":"2024-07-15T17:01:07.521790Z","shell.execute_reply":"2024-07-15T17:01:07.530396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a list with 12 months\n\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n\n# Iterating from each month to get average monthly temperature in train\nfor i in months:\n    cols_train = [col for col in df_train.columns if col.startswith(f'Average of {i}')] # putting all day average temperatures of a month in the list\n    cols_test = [col for col in df_test.columns if col.startswith(f'Average of {i}')]\n    df_train[f'Average of {i}'] = df_train[cols_train].sum(axis = 1)/len(cols_train) # adding all values in cols divide by length to get mean monthly temp.\n    df_test[f'Average of {i}'] = df_test[cols_test].sum(axis = 1)/len(cols_train)\n    df_train.drop(cols_train, axis = 1, inplace = True) # dropping those columns whose average are taken above\n    df_test.drop(cols_test, axis =1, inplace = True)","metadata":{"id":"8Lhyo7RET9pl","execution":{"iopub.status.busy":"2024-07-15T17:01:07.532787Z","iopub.execute_input":"2024-07-15T17:01:07.533145Z","iopub.status.idle":"2024-07-15T17:01:07.731599Z","shell.execute_reply.started":"2024-07-15T17:01:07.533115Z","shell.execute_reply":"2024-07-15T17:01:07.730426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_temp_cols = [col for col in df_train.columns if col.startswith('Average of')]\nnew_temp_colss = [col for col in df_test.columns if col.startswith('Average of')]\nprint(\"Train Set: \\n\",new_temp_cols)\nprint(\"Test Set: \\n\",new_temp_colss)","metadata":{"id":"M-0ZjVT-XKI2","outputId":"2e1fa7fd-44b7-411f-f672-ecf3907d9a7d","execution":{"iopub.status.busy":"2024-07-15T17:01:07.732877Z","iopub.execute_input":"2024-07-15T17:01:07.733181Z","iopub.status.idle":"2024-07-15T17:01:07.739582Z","shell.execute_reply.started":"2024-07-15T17:01:07.733157Z","shell.execute_reply":"2024-07-15T17:01:07.738460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"id":"0EbjQC8JYwz9","outputId":"2c5ca3be-5bf3-4419-cba5-9a5d76602ae3","execution":{"iopub.status.busy":"2024-07-15T17:01:07.741506Z","iopub.execute_input":"2024-07-15T17:01:07.741979Z","iopub.status.idle":"2024-07-15T17:01:07.752343Z","shell.execute_reply.started":"2024-07-15T17:01:07.741940Z","shell.execute_reply":"2024-07-15T17:01:07.751192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now check for inconsistencies with the breast cancer diagnosis.","metadata":{"id":"6d7FGewcY8xq"}},{"cell_type":"markdown","source":"Train","metadata":{"id":"UNDeEIlbawVS"}},{"cell_type":"code","source":"df_train.groupby(['breast_cancer_diagnosis_code', 'breast_cancer_diagnosis_desc'], as_index=False)['patient_id'].count()","metadata":{"id":"GjmcqNIAat_8","outputId":"760e133d-1688-458f-8e12-1859a132d508","execution":{"iopub.status.busy":"2024-07-15T17:01:07.753881Z","iopub.execute_input":"2024-07-15T17:01:07.754296Z","iopub.status.idle":"2024-07-15T17:01:07.777490Z","shell.execute_reply.started":"2024-07-15T17:01:07.754256Z","shell.execute_reply":"2024-07-15T17:01:07.776511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keep in mind that the last column is no. of patients in each diagnosis code rather than the patient id itself.\n\nWe see that there are some diagnosis codes that refer to male cancers despite all our patients being women as seen in patient_gender feature.\n\nWe'll convert the make diagnosis codes to their respective matching female diagnosis codes.\n","metadata":{"id":"WmaYtJtbbV81"}},{"cell_type":"code","source":"# Fixing  inconsistencies by recoding male/unspecified codes to female codes\ndf_train['breast_cancer_diagnosis_code'] = df_train['breast_cancer_diagnosis_code'].replace({\n    # Recode male to female\n    'C50122': 'C50112', 'C50221': 'C50211', 'C50421': 'C50411', 'C50922': 'C50912',\n\n    'C509': 'C5091'\n})","metadata":{"id":"_1m6it1Qbxiv","execution":{"iopub.status.busy":"2024-07-15T17:01:07.778708Z","iopub.execute_input":"2024-07-15T17:01:07.779020Z","iopub.status.idle":"2024-07-15T17:01:07.790398Z","shell.execute_reply.started":"2024-07-15T17:01:07.778993Z","shell.execute_reply":"2024-07-15T17:01:07.789336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's do the same check in test set","metadata":{"id":"yVayb91rcQmR"}},{"cell_type":"code","source":"df_test.groupby(['breast_cancer_diagnosis_code', 'breast_cancer_diagnosis_desc'], as_index = True)['patient_id'].count()","metadata":{"id":"UyzCeKOAcT_3","outputId":"83bdac6b-8981-452a-8cda-cda1f4a25aa5","execution":{"iopub.status.busy":"2024-07-15T17:01:07.791918Z","iopub.execute_input":"2024-07-15T17:01:07.792329Z","iopub.status.idle":"2024-07-15T17:01:07.807021Z","shell.execute_reply.started":"2024-07-15T17:01:07.792274Z","shell.execute_reply":"2024-07-15T17:01:07.805618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no inconsistencies with the diagnosis code in our test set.","metadata":{"id":"pAcq191adjBg"}},{"cell_type":"markdown","source":"Now, let's work on **replacing missing values** in our train and test sets","metadata":{"id":"R_TfZ7HqdoOA"}},{"cell_type":"code","source":"print(\"Missing values count in Train: \")\ndf_train.isnull().sum().sort_values(ascending = False).head(25) # counting the number of missing values in aech feature","metadata":{"id":"zF7nhjfVqmZk","outputId":"5f1797d6-4ae3-4815-88a6-34d51173c8b8","execution":{"iopub.status.busy":"2024-07-15T17:01:07.808612Z","iopub.execute_input":"2024-07-15T17:01:07.808967Z","iopub.status.idle":"2024-07-15T17:01:07.828177Z","shell.execute_reply.started":"2024-07-15T17:01:07.808938Z","shell.execute_reply":"2024-07-15T17:01:07.827048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Missing values count in Test: \")\ndf_test.isnull().sum().sort_values(ascending = False).head(20)","metadata":{"id":"1FYW69kGrkv9","outputId":"d7f16698-76c5-44e1-a098-7e7f884f1978","execution":{"iopub.status.busy":"2024-07-15T17:01:07.829917Z","iopub.execute_input":"2024-07-15T17:01:07.830259Z","iopub.status.idle":"2024-07-15T17:01:07.844298Z","shell.execute_reply.started":"2024-07-15T17:01:07.830221Z","shell.execute_reply":"2024-07-15T17:01:07.843031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apart from the 3 features with the most missing values, we see that other features especially in train set have a couple missing values(5 and under).\n\nLet's look into this by starting with **population based featured** and find ways to fill in the missing values.","metadata":{"id":"Nen0au8TsfrR"}},{"cell_type":"markdown","source":"For example: **'limited_english'**","metadata":{"id":"-U2TnFiEtFYz"}},{"cell_type":"code","source":"df_train[df_train.limited_english.isna()]","metadata":{"id":"BogZyZrjsu6J","outputId":"9e69ca6c-8593-4904-cb96-bdea34c3a712","execution":{"iopub.status.busy":"2024-07-15T17:01:07.845831Z","iopub.execute_input":"2024-07-15T17:01:07.846192Z","iopub.status.idle":"2024-07-15T17:01:07.877345Z","shell.execute_reply.started":"2024-07-15T17:01:07.846162Z","shell.execute_reply":"2024-07-15T17:01:07.875733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like all the patients with missing values in limited english are from patient zip 772.\n\nLet's get a subset with patients on zip 772 and see if there are any valid values.","metadata":{"id":"CkB6fRKUtBMt"}},{"cell_type":"code","source":"print(df_train[df_train.patient_zip3 == 772].limited_english.unique())\ndf_train[df_train.patient_zip3 == 772].head(10)","metadata":{"id":"kWFFIMbNtAxS","outputId":"4cf200e1-325e-4a7a-a67e-6d9440c55211","execution":{"iopub.status.busy":"2024-07-15T17:01:07.878880Z","iopub.execute_input":"2024-07-15T17:01:07.879241Z","iopub.status.idle":"2024-07-15T17:01:07.914427Z","shell.execute_reply.started":"2024-07-15T17:01:07.879187Z","shell.execute_reply":"2024-07-15T17:01:07.913284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it seems no patient in zip 772 has valid values in limited_english.\n\nWe also see that only 5 patients in our dataset are in zip 772. Let's see if we can leverage data from the same state(Texas)","metadata":{"id":"TAwS5wsTt5WE"}},{"cell_type":"code","source":"df_train[(df_train.patient_state == 'TX') & (~df_train.limited_english.isna())].head()","metadata":{"id":"EUpSuemGuncg","outputId":"ff7da4e2-ec63-4ff3-ea81-a8239db34857","execution":{"iopub.status.busy":"2024-07-15T17:01:07.916169Z","iopub.execute_input":"2024-07-15T17:01:07.916550Z","iopub.status.idle":"2024-07-15T17:01:07.947002Z","shell.execute_reply.started":"2024-07-15T17:01:07.916518Z","shell.execute_reply":"2024-07-15T17:01:07.945987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's reduce this into population related features so we can get a better look at the values.","metadata":{"id":"1L8H4q5DvORh"}},{"cell_type":"code","source":"df_train.shape","metadata":{"id":"PH-qxExyzzxW","outputId":"f287dcbc-70d8-4560-e5d6-b6e2e76aa078","execution":{"iopub.status.busy":"2024-07-15T17:01:07.948434Z","iopub.execute_input":"2024-07-15T17:01:07.948804Z","iopub.status.idle":"2024-07-15T17:01:07.955908Z","shell.execute_reply.started":"2024-07-15T17:01:07.948773Z","shell.execute_reply":"2024-07-15T17:01:07.954656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"popln_cols = df_train.loc[:, 'population':'veteran'].columns.to_list()\ndf_popln = df_train[['patient_zip3', 'patient_state'] + popln_cols].drop_duplicates().sort_values('patient_zip3')\nprint(df_popln.shape)\ndf_popln.head()","metadata":{"id":"-3TRDeOyvVzc","outputId":"851df63a-d355-4722-ef87-8b858c909be8","execution":{"iopub.status.busy":"2024-07-15T17:01:07.957862Z","iopub.execute_input":"2024-07-15T17:01:07.958294Z","iopub.status.idle":"2024-07-15T17:01:08.028865Z","shell.execute_reply.started":"2024-07-15T17:01:07.958261Z","shell.execute_reply":"2024-07-15T17:01:08.027796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's run the previos code to see limited_english values in Texas.","metadata":{"id":"th05AJPRv6SS"}},{"cell_type":"code","source":"df_popln[(df_popln.patient_state == 'TX') & (~df_popln.limited_english.isna())].head()","metadata":{"id":"8AiYrzn3wAdO","outputId":"26489868-8c72-4b04-cd54-05e51c2a57e8","execution":{"iopub.status.busy":"2024-07-15T17:01:08.030815Z","iopub.execute_input":"2024-07-15T17:01:08.031202Z","iopub.status.idle":"2024-07-15T17:01:08.060897Z","shell.execute_reply.started":"2024-07-15T17:01:08.031173Z","shell.execute_reply":"2024-07-15T17:01:08.059657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are values in limited_english feature. We could replace missing values with averages from the state or with values from the closest zip.","metadata":{"id":"ds8rtp7zwL5K"}},{"cell_type":"code","source":"df_popln[(df_popln.patient_state == 'TX') & (df_popln.patient_zip3 > 767)].head(10)","metadata":{"id":"0z2M080Qwey2","outputId":"7d48dd62-3373-4657-e1a7-a9c56f6bc423","execution":{"iopub.status.busy":"2024-07-15T17:01:08.062367Z","iopub.execute_input":"2024-07-15T17:01:08.062885Z","iopub.status.idle":"2024-07-15T17:01:08.099685Z","shell.execute_reply.started":"2024-07-15T17:01:08.062845Z","shell.execute_reply":"2024-07-15T17:01:08.098485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replacing missing values from 772 with those of nearest zips(770 or 773) may be riskier because sometimes there's a huge difference in population statistics between different zipcodes.\n\nFor example the population varies significantly btw 770 (33353.72\t) vs 772 ( 4459.00) as well as btw 773 (24751.20) and 772 ( 4459.00).\n\nSo, replacing with average for the state is a better move.","metadata":{"id":"R6-3Dr6Ew_u8"}},{"cell_type":"markdown","source":"Before doing that, let's check a couple more features to see if other missing values are also from the same zip.","metadata":{"id":"9u7-YvpYywPG"}},{"cell_type":"code","source":"df_train[df_train.home_ownership.isna()]","metadata":{"id":"w_t13rjbxxQ9","outputId":"c453a59f-9319-4924-e6f7-87fb365e4cb4","execution":{"iopub.status.busy":"2024-07-15T17:01:08.101037Z","iopub.execute_input":"2024-07-15T17:01:08.101378Z","iopub.status.idle":"2024-07-15T17:01:08.131578Z","shell.execute_reply.started":"2024-07-15T17:01:08.101351Z","shell.execute_reply":"2024-07-15T17:01:08.130440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train.family_size.isna()]","metadata":{"id":"2lFqrA5ry35A","outputId":"cd591f54-96df-4b31-fb99-eda27807d7e9","execution":{"iopub.status.busy":"2024-07-15T17:01:08.132909Z","iopub.execute_input":"2024-07-15T17:01:08.133259Z","iopub.status.idle":"2024-07-15T17:01:08.161184Z","shell.execute_reply.started":"2024-07-15T17:01:08.133230Z","shell.execute_reply":"2024-07-15T17:01:08.160149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train.rent_median.isna()]","metadata":{"id":"cBAyog4Oy_BN","outputId":"f90d19aa-8da8-4f16-84db-8c2c568ebeac","execution":{"iopub.status.busy":"2024-07-15T17:01:08.162245Z","iopub.execute_input":"2024-07-15T17:01:08.162533Z","iopub.status.idle":"2024-07-15T17:01:08.191255Z","shell.execute_reply.started":"2024-07-15T17:01:08.162508Z","shell.execute_reply":"2024-07-15T17:01:08.190149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can assume a lot of the population based missing values are from patients located in zip 772.\n\nWe'll replace missing values in all population based features with the mean from the state.","metadata":{"id":"j_pT5mPTzF9S"}},{"cell_type":"code","source":"# Replace missing values\n\n# creating a function\ndef mixed_replacement(df, group_col):\n    for column in df.columns:\n        if column != group_col:  # Exclude the group column\n            # If the column is numerical, then replace missing values with mean\n            if df[column].dtype in [np.dtype('float_'), np.dtype('int_')]:\n                mean_impute = df.groupby(group_col)[column].mean()\n                df[column] = df[column].fillna(df[group_col].map(mean_impute))\n\n            # If the column is categorical, then replace missing values with mode\n            else :\n                mode_impute = df.groupby(group_col)[column].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n                df[column] = df[column].fillna(df[group_col].map(mode_impute))\n\n    return df\n\n# Impute missing values\ndf_popln = mixed_replacement(df = df_popln, group_col='patient_state')","metadata":{"id":"w5Ii1AA0zMUT","execution":{"iopub.status.busy":"2024-07-15T17:01:08.193107Z","iopub.execute_input":"2024-07-15T17:01:08.193553Z","iopub.status.idle":"2024-07-15T17:01:08.291481Z","shell.execute_reply.started":"2024-07-15T17:01:08.193511Z","shell.execute_reply":"2024-07-15T17:01:08.290552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_popln.isnull().sum().sort_values(ascending = False).head()","metadata":{"id":"KhfIQRwg2Dqk","outputId":"dfd13f5c-a27c-4c70-dcef-a97047e1d360","execution":{"iopub.status.busy":"2024-07-15T17:01:08.292788Z","iopub.execute_input":"2024-07-15T17:01:08.293196Z","iopub.status.idle":"2024-07-15T17:01:08.307411Z","shell.execute_reply.started":"2024-07-15T17:01:08.293160Z","shell.execute_reply":"2024-07-15T17:01:08.306362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_popln.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:01:08.308948Z","iopub.execute_input":"2024-07-15T17:01:08.309645Z","iopub.status.idle":"2024-07-15T17:01:08.317599Z","shell.execute_reply.started":"2024-07-15T17:01:08.309602Z","shell.execute_reply":"2024-07-15T17:01:08.316437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's tackle missing values in **payer_type**\n\nAbout 13% of values are missing from payer_type. Let's quickly look at the distribution of the unique values in both train and test set.","metadata":{"id":"nqfpkyZt2u-x"}},{"cell_type":"code","source":"df_train['payer_type'].value_counts(dropna=False, normalize=True) # using normalize to return proportions rather than frequency","metadata":{"id":"WG1NrdD32yLJ","outputId":"6139451d-5d61-4b6b-e18b-ed6da0492224","execution":{"iopub.status.busy":"2024-07-15T17:01:08.318960Z","iopub.execute_input":"2024-07-15T17:01:08.319365Z","iopub.status.idle":"2024-07-15T17:01:08.332433Z","shell.execute_reply.started":"2024-07-15T17:01:08.319326Z","shell.execute_reply":"2024-07-15T17:01:08.331360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['payer_type'].value_counts(dropna=False, normalize=True)","metadata":{"id":"1OGqZKC97zRk","outputId":"90bc0d76-09ba-48b1-db5b-2b528582a7e5","execution":{"iopub.status.busy":"2024-07-15T17:01:08.333662Z","iopub.execute_input":"2024-07-15T17:01:08.333992Z","iopub.status.idle":"2024-07-15T17:01:08.347583Z","shell.execute_reply.started":"2024-07-15T17:01:08.333937Z","shell.execute_reply":"2024-07-15T17:01:08.346390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these distributions, a missing value could either indicate that the patient is either **uninsured** or using **commercial**(since it has the highest distribution)","metadata":{"id":"15R1d3_I8T54"}},{"cell_type":"code","source":"# For simplicity, let's assume missing values are 'COMMERCIAL'. We'll replace NaN with 'COMMERCIAL'\n\ndf_train['payer_type'] = df_train['payer_type'].fillna('COMMERCIAL')\ndf_test['payer_type'] = df_test['payer_type'].fillna('COMMERCIAL')","metadata":{"id":"6bQ6EpT38lJl","execution":{"iopub.status.busy":"2024-07-15T17:01:08.349251Z","iopub.execute_input":"2024-07-15T17:01:08.349677Z","iopub.status.idle":"2024-07-15T17:01:08.361059Z","shell.execute_reply.started":"2024-07-15T17:01:08.349646Z","shell.execute_reply":"2024-07-15T17:01:08.360035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum().sort_values(ascending = False).head(25)","metadata":{"id":"FObDvjC92bIJ","outputId":"829fb746-12af-4891-97ad-12cabda23191","execution":{"iopub.status.busy":"2024-07-15T17:01:08.362489Z","iopub.execute_input":"2024-07-15T17:01:08.362944Z","iopub.status.idle":"2024-07-15T17:01:08.387649Z","shell.execute_reply.started":"2024-07-15T17:01:08.362905Z","shell.execute_reply":"2024-07-15T17:01:08.386407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's merge the df_popln back into train and test sets.\n\n# To ensure that the number of rows remains the same after the merge,we will remove \n# the duplicates from df_popln, keeping only unique combinations of patient_zip3 and patient_state.\ndf_popln_unique = df_popln.drop_duplicates(subset=['patient_zip3', 'patient_state'])\n\nclean_train = df_train.drop(popln_cols, axis = 1).merge(\n    df_popln_unique, how ='left', on =['patient_zip3', 'patient_state']\n)\nclean_test = df_test.drop(popln_cols, axis = 1).merge(\n    df_popln_unique, how ='left', on =['patient_zip3', 'patient_state']\n)\n\nprint(clean_train.shape, df_train.shape, clean_test.shape,  df_test.shape)\nclean_train.head(10)","metadata":{"id":"i-H14h8P-Q96","outputId":"07a227d4-cb10-4394-87a1-e00930b84990","execution":{"iopub.status.busy":"2024-07-15T17:01:08.389187Z","iopub.execute_input":"2024-07-15T17:01:08.389580Z","iopub.status.idle":"2024-07-15T17:01:08.469783Z","shell.execute_reply.started":"2024-07-15T17:01:08.389530Z","shell.execute_reply":"2024-07-15T17:01:08.468511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train.isnull().sum().sort_values(ascending = False).head()","metadata":{"id":"_0O6y1FYDYvE","outputId":"26238025-1f28-4cb2-f862-c319c8674b4a","execution":{"iopub.status.busy":"2024-07-15T17:01:08.471155Z","iopub.execute_input":"2024-07-15T17:01:08.471716Z","iopub.status.idle":"2024-07-15T17:01:08.487350Z","shell.execute_reply.started":"2024-07-15T17:01:08.471681Z","shell.execute_reply":"2024-07-15T17:01:08.486296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"id":"vC8ZBEhz_O8t","outputId":"23408455-ba76-4b04-820f-c68d89409304","execution":{"iopub.status.busy":"2024-07-15T17:01:08.488637Z","iopub.execute_input":"2024-07-15T17:01:08.490875Z","iopub.status.idle":"2024-07-15T17:01:08.497961Z","shell.execute_reply.started":"2024-07-15T17:01:08.490838Z","shell.execute_reply":"2024-07-15T17:01:08.496849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train.shape","metadata":{"id":"6lDqdlQp_wj_","outputId":"9a7e0607-3cd8-474d-a14a-b9205adb2502","execution":{"iopub.status.busy":"2024-07-15T17:01:08.499267Z","iopub.execute_input":"2024-07-15T17:01:08.499679Z","iopub.status.idle":"2024-07-15T17:01:08.508455Z","shell.execute_reply.started":"2024-07-15T17:01:08.499645Z","shell.execute_reply":"2024-07-15T17:01:08.507289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still have to work something on 'bmi' and 'patient_race' regarding the missing values.\n\n**bmi** - We'll replace missing values in bmi with 0\n\n**patient_race** - We'll replace missing values with 'N/A'","metadata":{"id":"cbmL10dSB73h"}},{"cell_type":"code","source":"# Replacing all Nan values in 'bmi' with 0\nclean_train['bmi'] = clean_train['bmi'].fillna(0)\nclean_test['bmi'] = clean_test['bmi'].fillna(0)\n\n# Replacing all Nan values in 'patient_race' with 'N/A'\n\nclean_train['patient_race'] = clean_train['patient_race'].fillna('N/A')\nclean_test['patient_race'] = clean_test['patient_race'].fillna('N/A')","metadata":{"id":"XYAvGJ7CCoso","execution":{"iopub.status.busy":"2024-07-15T17:01:08.510007Z","iopub.execute_input":"2024-07-15T17:01:08.510486Z","iopub.status.idle":"2024-07-15T17:01:08.523187Z","shell.execute_reply.started":"2024-07-15T17:01:08.510445Z","shell.execute_reply":"2024-07-15T17:01:08.521896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check if there are still any missing values in our data set\nclean_train.isnull().sum().sort_values(ascending = False).head()\n","metadata":{"id":"wq1s78T97NqP","outputId":"c3841a18-be65-4039-9c9b-a77e132aa20b","execution":{"iopub.status.busy":"2024-07-15T17:01:08.525012Z","iopub.execute_input":"2024-07-15T17:01:08.525402Z","iopub.status.idle":"2024-07-15T17:01:08.541633Z","shell.execute_reply.started":"2024-07-15T17:01:08.525369Z","shell.execute_reply":"2024-07-15T17:01:08.540346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_test.isnull().sum().sort_values(ascending = False).head(40)","metadata":{"id":"JSln7sz28yL8","outputId":"2e31929c-9c8e-468c-8eb3-0b0d982dd55a","execution":{"iopub.status.busy":"2024-07-15T17:01:08.543043Z","iopub.execute_input":"2024-07-15T17:01:08.543386Z","iopub.status.idle":"2024-07-15T17:01:08.556833Z","shell.execute_reply.started":"2024-07-15T17:01:08.543357Z","shell.execute_reply":"2024-07-15T17:01:08.555584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mmh, looks like we still have some missing values in the test set. We'll work on this first before moving to the next stage","metadata":{"id":"EXX2N7J49DKP"}},{"cell_type":"code","source":"import warnings\ndef ignore_warning():\n    # Code that generates warnings\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        ignore_warning()\n    \n# Saving all columns with null value\nna_cols = clean_test.columns[clean_test.isna().sum() > 0].tolist()\n\n# Filling these columns with null value with mean of each column\nfor column in na_cols:\n    clean_test[column].fillna(clean_test[column].mean(), inplace=True)\n\n# checking null values presence after imputation of mean\nprint(clean_test.isnull().sum().sort_values(ascending = False).head())\nprint(clean_test.columns[clean_test.isna().sum() > 0])","metadata":{"id":"4-EU0kkKAsEI","outputId":"e2c28118-ad2e-49d6-a3cf-e75a6620aa46","execution":{"iopub.status.busy":"2024-07-15T17:01:08.558278Z","iopub.execute_input":"2024-07-15T17:01:08.558707Z","iopub.status.idle":"2024-07-15T17:01:08.604573Z","shell.execute_reply.started":"2024-07-15T17:01:08.558665Z","shell.execute_reply":"2024-07-15T17:01:08.603292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"id":"JhCJ4RjJ8KDX"}},{"cell_type":"markdown","source":"After pre processing our data by filling in missing values and replacing outliers, we'll now do some feature engineering to make sure all our data is readable/recognizable by our machine learning model. Here's a summary of what we'll do:\n\n\n\n*   Create age ranges of equal sizes ex: under 29, 30-39, 40-49 etc.\n*   Convert other categorical features using suitable encoding method\n\n\nEncoding categorical variables means transforming the categorical variables(be it in text or numerical form) into a numerical format that is compatible with machine learning algorithm.\n\nSome widely used label encoding methods are:\n\n1.   **One-hot Encoding** - represents categorical variables as binary vectors, which  increases dimensionality of the data set. For each category in a categorical column, a new binary column is created. The binary column will have a value of 1 if the class is present, else it will be zero.\n2.   **Label Encoding** - converts the unique labels of a categorical variable into numeric form without affecting the dimensionality of the data set.\n3.   **Ordinal Encoding** - this is similar to label encoding but allows you to explicitly define the mapping between categories and integer labels. This is especially useful when there is a clear and predefined ordinal relationship.\n4.   **Count/Frequency Encoding** - replaces each category with the count of how many times it appears in the dataset. This encoding technique can be useful when there’s a correlation between the frequency of a category and the target variable. Also applicable for categorical features with a lot of categories. The count_encoder should be fit only on the train dataset. The fitted object should be used to transform test and out of time (OOT) datasets.\n5.   **Target(Mean) Encoding** - involves replacing each category with the mean (or some other statistic) of the target variable for that category.\n\nNote:\n\nThe main differences between Nominal Data and Ordinal Data are: \n\nWhile Ordinal Data has some predetermined or natural order, Nominal Data doesn't; it usually has some categories like if an animal is a mammal or reptile etc. Nominal data is qualitative or categorical data, while Ordinal data is considered “in-between” qualitative and quantitative data.\n\nLearn more about these and other methods [here](https://medium.com/anolytics/all-you-need-to-know-about-encoding-techniques-b3a0af68338b).\n","metadata":{"id":"g-TyoFLO8OP2"}},{"cell_type":"code","source":"# putting patient age ranges into bin using pd.cut function\n# pd.cut segments and sorts data values into bins. It is also useful for going from a continuous variable to a categorical variable\nclean_train['age_group'] = pd.cut(clean_train['patient_age'], right=False, bins=[0, 30, 40, 50, 60, 70, 80, 90, np.inf], labels=[0,1,2,3,4,5,6,7]).astype(int)\nclean_test['age_group']  = pd.cut(clean_test['patient_age'], right=False, bins=[0, 30, 40, 50, 60, 70, 80, 90, np.inf], labels=[0,1,2,3,4,5,6,7]).astype(int)","metadata":{"id":"en6hsBdJDHOc","execution":{"iopub.status.busy":"2024-07-15T17:01:08.605852Z","iopub.execute_input":"2024-07-15T17:01:08.606179Z","iopub.status.idle":"2024-07-15T17:01:08.620193Z","shell.execute_reply.started":"2024-07-15T17:01:08.606150Z","shell.execute_reply":"2024-07-15T17:01:08.618814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check if our pd.cut function worked\nclean_train['age_group'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:01:08.621958Z","iopub.execute_input":"2024-07-15T17:01:08.622403Z","iopub.status.idle":"2024-07-15T17:01:08.632530Z","shell.execute_reply.started":"2024-07-15T17:01:08.622363Z","shell.execute_reply":"2024-07-15T17:01:08.631002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's drop the 'breast_cancer_diagnosis_desc' and 'patient_id' columns since they won't be helpful in model training\n\n# Dropping 'breast_cancer_diagnosis_desc'\nclean_train.drop('breast_cancer_diagnosis_desc', axis = 1, inplace = True)\nclean_test.drop('breast_cancer_diagnosis_desc', axis = 1, inplace = True)\n# Dropping 'patient_id'\nclean_train.drop('patient_id', axis = 1, inplace = True)\nclean_test.drop('patient_id', axis = 1, inplace = True)","metadata":{"id":"JwokPWNfC8cE","execution":{"iopub.status.busy":"2024-07-15T17:01:08.633988Z","iopub.execute_input":"2024-07-15T17:01:08.634369Z","iopub.status.idle":"2024-07-15T17:01:08.658201Z","shell.execute_reply.started":"2024-07-15T17:01:08.634328Z","shell.execute_reply":"2024-07-15T17:01:08.656966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding BMI into buckets\n# Since we already put 0 for missing values in bmi, we'll start with label 1 for 0 < 'bmi' < 18.5\n\n# clean_train['bmi_range'] +=  np.where(0 < clean_train['bmi'] < 18.5, 1)\n# clean_train['bmi_range'] += np.where(18.5 <= clean_train['bmi'] < 25, 2)\n# clean_train['bmi_range'] += np.where(25 <= clean_train['bmi'] < 30, 3)\n# clean_train['bmi_range'] += np.where(clean_train['bmi'] >= 30, 4)\n","metadata":{"id":"1sVkuMGOPsGo","execution":{"iopub.status.busy":"2024-07-15T17:01:08.659777Z","iopub.execute_input":"2024-07-15T17:01:08.660273Z","iopub.status.idle":"2024-07-15T17:01:08.666092Z","shell.execute_reply.started":"2024-07-15T17:01:08.660233Z","shell.execute_reply":"2024-07-15T17:01:08.664843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's standardize numerical columns and encode the categorical variables we just saw above\n\n# First, let's find all the numerical columns\nnumeric_columns = clean_train.drop(['metastatic_diagnosis_period'], axis=1).select_dtypes(include = ['number']).columns\nprint(\"Numeric columns:\\n\", numeric_columns)\nnumeric_columns_test = clean_test.select_dtypes(include = ['number']).columns\n\n# Standardizing the numerical columns\nscaler = StandardScaler()\nclean_train[numeric_columns] = scaler.fit_transform(clean_train[numeric_columns])\n# print(\"Standardized Train DataFrame:\\n\", clean_train.head(5))\nclean_test[numeric_columns_test] = scaler.fit_transform(clean_test[numeric_columns_test])\n# print(\"Standardized Test DataFrame:\\n\", clean_test.head(5))\n\n# Finding all non-numeric columns\nnon_numeric_columns = clean_train.select_dtypes(include=['object']).columns\nprint(\"Non-numeric columns:\", non_numeric_columns)\n\n# Converting categorical features to numerical values using LabelEncoder\nlabel_encoders_train = {}\nlabel_encoders_test = {}\nfor column in non_numeric_columns:\n    le_train = LabelEncoder()\n    le_test = LabelEncoder()\n    clean_train[column] = le_train.fit_transform(clean_train[column])\n    label_encoders_train[column] = le_train\n    clean_test[column] = le_test.fit_transform(clean_test[column])\n    label_encoders_test[column] = le_test\n    \nprint(\"Train DataFrame after Label Encoding:\\n\", clean_train.head(5))","metadata":{"id":"Ng6w8GkeMUEx","execution":{"iopub.status.busy":"2024-07-15T17:01:08.667816Z","iopub.execute_input":"2024-07-15T17:01:08.668275Z","iopub.status.idle":"2024-07-15T17:01:08.816398Z","shell.execute_reply.started":"2024-07-15T17:01:08.668232Z","shell.execute_reply":"2024-07-15T17:01:08.815272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see if the label encoding worked.\nfor col in clean_train.columns:\n  if clean_train[col].dtypes == 'object':\n    print(col)\n  exit()\n\nprint(\"All categorical columns in train data are encoded successfully!\")\n\nfor col in clean_test.columns:\n  if clean_test[col].dtypes == 'object':\n    print(col)\n  exit()\nprint(\"All categorical columns in test data are encoded successfully!\")   ","metadata":{"id":"UImARg0bOWd1","outputId":"d7c9de10-fd7f-44e4-dc8d-6238875e2f8b","execution":{"iopub.status.busy":"2024-07-15T17:01:08.818092Z","iopub.execute_input":"2024-07-15T17:01:08.818438Z","iopub.status.idle":"2024-07-15T17:01:08.832682Z","shell.execute_reply.started":"2024-07-15T17:01:08.818408Z","shell.execute_reply":"2024-07-15T17:01:08.831506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Clean train shape: \",clean_train.shape)\nprint(\"Clean test shape: \",clean_test.shape)","metadata":{"id":"445BOuFhaxQ8","outputId":"7158664d-31a8-48ba-df41-7d965e1fd1eb","execution":{"iopub.status.busy":"2024-07-15T17:01:08.834110Z","iopub.execute_input":"2024-07-15T17:01:08.834547Z","iopub.status.idle":"2024-07-15T17:01:08.848538Z","shell.execute_reply.started":"2024-07-15T17:01:08.834507Z","shell.execute_reply":"2024-07-15T17:01:08.847268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's do another check for non numeric columns:\nnon_numeric_columns_train = clean_train.select_dtypes(include=['object']).columns\nnon_numeric_columns_test = clean_test.select_dtypes(include=['object']).columns\nprint(\"Non-numeric columns in train:\\n\", non_numeric_columns_train)\nprint(\"Non-numeric columns in test:\\n\", non_numeric_columns_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:01:08.849925Z","iopub.execute_input":"2024-07-15T17:01:08.850455Z","iopub.status.idle":"2024-07-15T17:01:08.860055Z","shell.execute_reply.started":"2024-07-15T17:01:08.850392Z","shell.execute_reply":"2024-07-15T17:01:08.859015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{"id":"EMLCPfXmceS9"}},{"cell_type":"markdown","source":"Feature selection is the process of identifying and selecting relevant features from large set of features to boost the predictive power and accuracy of the model.\n\nWe know that our dataset has a very high dimensionality, this can decrease model efficiency as it will take a long time to train and can also decrease perfomance as it will be prone to overfitting.\n\nFeature selection will help us:\n- Decrease the dimension of our dataset\n- Speed up machine learning model\n- Decrease our likelihood of overfitting.\n- Improve ability to comprehend our model results\n\nLet's quickly review the techniques we can use for feature selection:\n\n1.   **Filter Methods**\n\n  These methods select features from the dataset irrespective of the use of any machine learning algorithm by considering the relationship between features and the target variable to compute the importance of features.\n\n  Pros:\n  - Fast and inexpensive computation\n  - Great for removing duplicated, correlated, redundant features\n\n  Cons:\n  - They do not remove multicollinearity. Selection of feature is evaluated individually which can sometimes help when features are in isolation (don’t have a dependency on other features) but will lag when a combination of features can lead to increase in the overall performance of the model.\n\n  Some of the filter methods are:\n  - Information Gain – The amount of information provided by the feature for identifying the target value and measures reduction in the entropy values. Information gain of each attribute is calculated considering the target values for feature selection.\n  - Chi-square test — Chi-square method (X2) is generally used to test the relationship between categorical variables. It evaluates the independence of a categorical characteristic from the target variable, establishing its relevance and usefulness for classification tasks.\n  - Fisher’s Score – Fisher’s Score selects each feature independently according to their scores under Fisher criterion leading to a suboptimal set of features. The larger the Fisher’s score is, the better is the selected feature.\n  - Correlation Coefficient – Pearson’s Correlation Coefficient is a measure of quantifying the association between the two continuous variables and the direction of the relationship with its values ranging from -1 to 1.\n  - Variance Threshold – It is an approach where all features are removed whose variance doesn’t meet the specific threshold. By default, this method removes features having zero variance. The assumption made using this method is higher variance features are likely to contain more information.\n  - Mean Absolute Difference (MAD) – This method is similar to variance threshold method but the difference is there is no square in MAD. This method calculates the mean absolute difference from the mean value.\n2.   **Wrapper Methods**\n\n  Wrapper methods also reffered to as also referred to as greedy algorithms,  generate models with a subsets of feature and gauge their model performances. Based on the conclusions made from training in prior to the model, addition and removal of features takes place. Stopping criteria for selecting the best subset are usually pre-defined by the person training the model such as when the performance of the model decreases or a specific number of features has been achieved\n\n  Pro:\n  - The main advantage of wrapper methods over the filter methods is that they provide an optimal set of features for training the model, thus resulting in better accuracy than the filter methods.\n  \n  Con:\n  - They are computationally more expensive.\n\n  Some techniques used are:\n\n  - Forward selection – This method is an iterative approach where we initially start with an empty set of features and keep adding a feature which best improves our model after each iteration. The stopping criterion is till the addition of a new variable does not improve the performance of the model.\n  - Backward elimination – This method is also an iterative approach where we initially start with all features and after each iteration, we remove the least significant feature. The stopping criterion is till no improvement in the performance of the model is observed after the feature is removed.\n  - Bi-directional elimination – This method uses both forward selection and backward elimination technique simultaneously to reach one unique solution.\n  Exhaustive selection – This technique is considered as the brute force approach for the evaluation of feature subsets. It creates all possible subsets and builds a learning algorithm for each subset and selects the subset whose model’s performance is best.\n  - Recursive elimination – This greedy optimization method selects features by recursively considering the smaller and smaller set of features. The estimator is trained on an initial set of features and their importance is obtained using feature_importance_attribute. The least important features are then removed from the current set of features till we are left with the required number of features.\n\n\n\n3.   **Embedded Methods**\n\n  With these, the feature selection algorithm is blended as part of the learning algorithm, thus having its own built-in feature selection methods.\n  \n  Pro:\n  - Are faster like those of filter methods\n  - More accurate than the filter methods\n  - They take into consideration a combination of features as well.\n\n  Some techniques used are:\n\n  - Regularization – This method adds a penalty to different parameters of the machine learning model to avoid over-fitting of the model. This approach of feature selection uses Lasso (L1 regularization) and Elastic nets (L1 and L2 regularization). The penalty is applied over the coefficients, thus bringing down some coefficients to zero. The features having zero coefficient can be removed from the dataset.\n  - Tree-based methods – These methods such as Random Forest, Gradient Boosting,CatBoosting provide us feature importance as a way to select features as well. Feature importance tells us which features are more important in making an impact on the target feature.\n","metadata":{"id":"frUb5f0hcnnY"}},{"cell_type":"markdown","source":"After reviewing all of the different feature selction techniques, I am more inclined to use one of the embedded techniques because of their accuracy and speed considering the dimension of my dataset.\n\nI'll be using Lasso Regression.\n\nLASSO stands for Least Absolute Shrinkage And Selection Operator regularization.\n\nLasso is a form of regularization for linear regression models. Regularization is a statistical method to reduce errors caused by overfitting on training data.\n\nLasso does feature selection through its L1 penalty term that minimizes the size of all coefficients and allows any coefficient to go to the value of zero, effectively removing input features from the model.\n\nLasso adds a penalty term to the residual sum of squares (RSS), which is then multiplied by the regularization parameter (lambda or λ). This regularization parameter controls the amount of regularization applied. Larger values of lambda increase the penalty, shrinking more of the coefficients towards zero, which subsequently reduces the importance of (or altogether eliminates) some of the features from the model, which results in automatic feature selection. Conversely, smaller values of lambda reduce the effect of the penalty, retaining more features within the model.\n\n\n(Geeking a little)The computation of the lasso solution is a quadratic programming problem, and can be tackled by standard numerical analysis algorithms.","metadata":{"id":"edX33z_SJNL2"}},{"cell_type":"code","source":"# Since our model is already split into train and test sets, we will split our train data into train and validation then predict on the test set.\n\n# Let's split the data into features and target\nX = clean_train.drop('metastatic_diagnosis_period', axis = 1).values\ny = clean_train['metastatic_diagnosis_period'].values\n\n\n# Train Test Split (but really tgrain validation split since we have test set to predict on)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 42, stratify = y) \n# When we use an integer for random_state, the function will produce the same results across different executions.\n#  The results are only changed if we change the integer value.\n\nprint(\"Shape of Train Features: {}\".format(X_train.shape))\nprint(\"Shape of Validation Features: {}\".format(X_val.shape))\nprint(\"Shape of Train Target: {}\".format(y_train.shape))\nprint(\"Shape of Validation Target: {}\".format(y_val.shape))","metadata":{"id":"pLr7xwPKM9sY","execution":{"iopub.status.busy":"2024-07-15T17:01:08.861463Z","iopub.execute_input":"2024-07-15T17:01:08.862357Z","iopub.status.idle":"2024-07-15T17:01:08.906675Z","shell.execute_reply.started":"2024-07-15T17:01:08.862326Z","shell.execute_reply":"2024-07-15T17:01:08.905513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use **GridSearchCV** to determine optimal hyperparameters for our lasso regression model.\n\nChoosing appropriate values for the alpha parameter in Lasso regression, especially with a large dataset, requires some strategic steps.\n\nThe alpha parameter controls the strength of the regularization: a higher value means more regularization, and a lower value means less\n\nHere's a systematic approach to setting up a grid for alpha:\n\n1. Understand the Scale of Your Data:\nWe standardized our data beforehand because Lasso regression is sensitive to the scale of the features.\n\n2. Initial Exploratory Search:\nWe'll perform an initial coarse search over a wide range of alpha values using a logarithmic scale. This helps identify a general region where good alpha values might lie.\n\n3. Refine the Search:\nOnce we identify a promising region, we'll perform a finer search in that range.","metadata":{}},{"cell_type":"code","source":"# Using GridSearchCV to find the best hyperparameters.\n\n# Initial coarse GridSearch over a wide range of alpha values\ninitial_param_grid = {\n    'alpha': np.logspace(-6, 6, 13)  # Wide range from 1e-6 to 1e+6\n}\n\n\n# # parameters to be tested on GridSearchCV\n# params = {\"alpha\":np.arange(0.00001, 10, 500)}\n\n# # Number of Folds and adding the random state for replication\n# kf=KFold(n_splits=5,shuffle=True, random_state=42)\n\n# Initializing the lasso regression model\nlasso = Lasso(max_iter = 30000)\n\n# # GridSearchCV with model, params and folds.\n# lasso_cv=GridSearchCV(lasso, param_grid=params, cv=kf)\n# lasso_cv.fit(X, y)\n# print(\"Best Params {}\".format(lasso_cv.best_params_))\n\n\n\n# Setting up our initial GridSearchCV\ninitial_grid_search = GridSearchCV(estimator = lasso, param_grid = initial_param_grid, cv = 5, scoring = 'neg_mean_squared_error', n_jobs = -1)\n\n# Fitting the model with the initial grid\ninitial_grid_search.fit(X_train, y_train)\n\n# Getting the best alpha from the initial search\nbest_initial_alpha = initial_grid_search.best_params_['alpha']\nprint(\"Best alpha from initial search:\", best_initial_alpha)\n\n# Defining a finer grid around the best alpha from the initial search\nfine_param_grid = {\n    'alpha': np.linspace(best_initial_alpha / 10, best_initial_alpha * 10, 20)\n}\n\n# Setting up fine GridSearchCV\nfine_grid_search = GridSearchCV(lasso, fine_param_grid, cv = 5, scoring = 'neg_mean_squared_error', n_jobs = -1)\n\n# Fitting the model with the fine_grid_search\nfine_grid_search.fit(X_train, y_train)\n\n# Getting the best parameters and the best score from the fine search\nbest_params = fine_grid_search.best_params_\nbest_score = fine_grid_search.best_score_\n\nprint(\"Best Parameters from fine search:\", best_params)\nprint(\"Best Score from fine search:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:01:08.908136Z","iopub.execute_input":"2024-07-15T17:01:08.908484Z","iopub.status.idle":"2024-07-15T17:05:32.124684Z","shell.execute_reply.started":"2024-07-15T17:01:08.908455Z","shell.execute_reply":"2024-07-15T17:05:32.121473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Lasso Regressor to plot the best features\n\n# Calling the model with the best parameter\n# From the above print statements we got best parameter as {'alpha': 0.006210526315789474}\nfine_lasso = Lasso(alpha=0.006210526315789474)\nfine_lasso.fit(X_train, y_train)\n\n# Using np.abs() to make coefficients positive.  \nfine_lasso_coef = np.abs(fine_lasso.coef_)\n\nfeature_names = clean_train.drop('metastatic_diagnosis_period', axis = 1).columns\n\nprint(\"Number of features in the lasso regression: \", fine_lasso.n_features_in_)\n# print(\"Names of features in the lasso regression:\\n \", fine_lasso.feature_names_in_)\n\nprint(fine_lasso_coef)\n\n# Plotting the Column Names and Importance of Columns. \nplt.bar(feature_names, fine_lasso_coef)\nplt.xticks(rotation=90)\nplt.grid()\nplt.title(\"Feature Selection Based on Lasso\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importance\")\nplt.ylim(0, 0.15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:32.126676Z","iopub.execute_input":"2024-07-15T17:05:32.127815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot and print out statement above, we see there are a lot of features with a coefficient of zero. This means they are not providing much value to our model. Let's find out what features are shrunk to 0 and which features were not.","metadata":{}},{"cell_type":"code","source":"# let's get the top 20 coefficients in lasso regression and print their respective features\n# we can use this to get the name of the features:\n# feature_names_in_ndarray of shape (n_features_in_,)\n# Names of features seen during fit. Defined only when X has feature names that are all strings.\n\n# and possibly this to get the number of features:\n# n_features_in_int\n# Number of features seen during fit.\n\n# Here's the documentation\n# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:34.176305Z","iopub.execute_input":"2024-07-15T17:05:34.176659Z","iopub.status.idle":"2024-07-15T17:05:34.181442Z","shell.execute_reply.started":"2024-07-15T17:05:34.176630Z","shell.execute_reply":"2024-07-15T17:05:34.180315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame to display feature names and their corresponding coefficients\ncoef_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Coefficient': fine_lasso_coef\n})\n\n# Sort by the (absolute) value of the coefficient\ncoef_df = coef_df.sort_values(by='Coefficient', ascending=False)\n\n# Display the top 20 features\ntop_20_features = coef_df.head(20) # top 20 features that Lasso did not shrink to zero\nprint(\"Top 20 features based on absolute coefficient values:\\n\", top_20_features['Feature'].tolist())\n\n# Display some of the features that lasso shrinked to zero\nbottom_20_features = coef_df.tail(20)\nprint(\"Some of the features that shrunk to zero:\\n\", bottom_20_features['Feature'].tolist())\n\n\n# Plotting the top 20 features with their coefficients\nplt.figure(figsize=(10, 8))\nplt.barh(top_20_features['Feature'], top_20_features['Coefficient'], color='b')\nplt.xlabel('Coefficient Value')\nplt.ylabel('Feature')\nplt.title('Top 20 Features by Coefficient Value')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest value at the top\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:34.182967Z","iopub.execute_input":"2024-07-15T17:05:34.183800Z","iopub.status.idle":"2024-07-15T17:05:34.586545Z","shell.execute_reply.started":"2024-07-15T17:05:34.183759Z","shell.execute_reply":"2024-07-15T17:05:34.585394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will create a new data set with the top 20 features for model training","metadata":{}},{"cell_type":"code","source":"model_df = clean_train[top_20_features['Feature'].tolist()] #minimizing clean_train to only 20 features\nprint(model_df.shape) #we should see 20 columns","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:34.588058Z","iopub.execute_input":"2024-07-15T17:05:34.588792Z","iopub.status.idle":"2024-07-15T17:05:34.598500Z","shell.execute_reply.started":"2024-07-15T17:05:34.588751Z","shell.execute_reply":"2024-07-15T17:05:34.597168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{"id":"7X8y01mpci-C"}},{"cell_type":"code","source":"# We'll be using random forest to train our model so that it can capture non linear relationships\n# We'll also do another fit using ridge regression since sometimes trees can lead to overfitting\n\n# Let's split our data again\nX_model = model_df # these are our top 20 columns\ny_model = clean_train['metastatic_diagnosis_period'].values\n\n\n# Train Test Split (but really train validation split since we have test set to predict on)\nX_train, X_val, y_train, y_val = train_test_split(X_model, y_model, test_size = 0.20, random_state = 42, stratify = y) \n# When we use an integer for random_state, the function will produce the same results across different executions.\n# The results are only changed if we change the integer value.\n\nprint(\"Shape of Train Features: {}\".format(X_train.shape))\nprint(\"Shape of Validation Features: {}\".format(X_val.shape))\nprint(\"Shape of Train Target: {}\".format(y_train.shape))\nprint(\"Shape of Validation Target: {}\".format(y_val.shape))\nprint(\"\\n\")\n\n# Let's create a dictionary with our machine learning models\nmodel_dict = {\n    \"Ridge\": (Ridge(), \"regression\"),\n    \"Random Forest Regressor\": (RandomForestRegressor(), \"regression\"),\n}\n# Let's start with Ridge\nridge = Ridge()\nridge.fit(X_train, y_train) # fitting the model\nprint(\"The ridge regressor is successfully fitted:)\")\n\ny_pred_ridge = ridge.predict(X_val) #getting predictions\n\n# Performance Metrics for Ridge Regression\nprint(\"Performance metrics for Ridge regressor: \")\n# Mean Squared Error\nridge_mse = mean_squared_error(y_val, y_pred_ridge)\nprint(\"Ridge Mean Squared Error: \", ridge_mse)\n# R-squared score\nridge_r2 = r2_score(y_val, y_pred_ridge)\nprint(\"Ridge r2 score: \", ridge_r2)\nprint(\"\\n\")\n\n\n# Now let's run the Random Forest model\nforest = RandomForestRegressor(random_state=42,max_depth=6)\nforest.fit(X_train, y_train)\nprint(\"The random forest regressor is successfully fitted:)\")\ny_pred_forest = forest.predict(X_val)\n\n# Performance Metrics for Random Forest\nprint(\"Performance metrics for Random forest regressor: \")\n# Mean Squared Error\nforest_mse = mean_squared_error(y_val, y_pred_forest)\nprint(\"Forest Mean Squared Error: \", forest_mse)\n# R-squared score\nforest_r2 = r2_score(y_val, y_pred_forest)\nprint(\"Forest r2 score: \", forest_r2)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:34.600059Z","iopub.execute_input":"2024-07-15T17:05:34.600411Z","iopub.status.idle":"2024-07-15T17:05:38.960618Z","shell.execute_reply.started":"2024-07-15T17:05:34.600375Z","shell.execute_reply":"2024-07-15T17:05:38.959638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_val)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:38.962045Z","iopub.execute_input":"2024-07-15T17:05:38.962415Z","iopub.status.idle":"2024-07-15T17:05:39.214956Z","shell.execute_reply.started":"2024-07-15T17:05:38.962388Z","shell.execute_reply":"2024-07-15T17:05:39.213945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_pred_forest)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:39.216236Z","iopub.execute_input":"2024-07-15T17:05:39.216700Z","iopub.status.idle":"2024-07-15T17:05:39.459817Z","shell.execute_reply.started":"2024-07-15T17:05:39.216659Z","shell.execute_reply":"2024-07-15T17:05:39.458843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the output above we see that r2 score for random forest is higher than ridge regression. \n\nMeaning, the forest model has a higher percentage of explaining variance in our independent/outcome variable.\n\nOn the other hand, we see that the MSE for forest model is lower than of ridge.\n\nMeaning, a smaller percentage of data points are dispersed widely around the mean in the random forest model compared to ridge model.","metadata":{}},{"cell_type":"markdown","source":"Given the 2 comparisons,we'll be picking random forest as our final model. We still have room for more improvement in our forest model, so we'll perform model validation to possibly increase our r2 score and decrease our MSE.","metadata":{}},{"cell_type":"markdown","source":"# Model Evaluation and Submission","metadata":{}},{"cell_type":"markdown","source":"After seeing how model works by validating our train data, we'll now use the model to predict on the test data.","metadata":{}},{"cell_type":"code","source":"# Using random forest model\nsubmission = solution_template.copy()\n# Minimizing clean_test to only 20 features\nmodel_test = clean_test[top_20_features['Feature'].tolist()]\n# Predicting on test set\nsubmission['metastatic_diagnosis_period'] = forest.predict( model_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:39.461424Z","iopub.execute_input":"2024-07-15T17:05:39.461871Z","iopub.status.idle":"2024-07-15T17:05:39.501069Z","shell.execute_reply.started":"2024-07-15T17:05:39.461833Z","shell.execute_reply":"2024-07-15T17:05:39.500120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:39.502324Z","iopub.execute_input":"2024-07-15T17:05:39.502663Z","iopub.status.idle":"2024-07-15T17:05:39.529748Z","shell.execute_reply.started":"2024-07-15T17:05:39.502635Z","shell.execute_reply":"2024-07-15T17:05:39.528759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['metastatic_diagnosis_period'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T17:05:39.530680Z","iopub.execute_input":"2024-07-15T17:05:39.530981Z","iopub.status.idle":"2024-07-15T17:05:39.902423Z","shell.execute_reply.started":"2024-07-15T17:05:39.530955Z","shell.execute_reply":"2024-07-15T17:05:39.901319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a surge of frequency at the around the 5o mark. It seems that it takes under 100 days for most of the diagnoses.","metadata":{}},{"cell_type":"markdown","source":"# Submission and Notes","metadata":{"id":"VXOAXz2XcpYF"}},{"cell_type":"markdown","source":"Next steps to consider:\n* Fine tuning the model\n* Using multiple ensemble models\n* Creating more visualizations for the 200-250 range for metastatic_diagnosis_period.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}